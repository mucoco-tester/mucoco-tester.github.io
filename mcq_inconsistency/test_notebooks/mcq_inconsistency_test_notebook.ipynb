{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## MuCoCo MCQ Inconsistency CodeMMLU Benchmark Testing\n",
    "\n",
    "This notebook is used for running experiments for MuCoCo MCQ inconsistency tasks on CodeMMLU benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "par_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(par_dir)\n",
    "sys.path.append(proj_dir)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcq_inconsistency.mcq_inconsistency_tester import LLMMCQInconsistencyTester\n",
    "from mcq_inconsistency.prompt_templates.prompt_template import MCQInconsistencyPromptTemplate, ReasoningMCQInconsistencyPromptTemplate\n",
    "from utility.constants import CodeMMLU, LexicalMutations, SyntacticMutations, LogicalMutations, PromptTypes, ReasoningModels, NonReasoningModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "FEW_SHOT = PromptTypes.FEW_SHOT\n",
    "\n",
    "## Declaring Benchmark Constants\n",
    "CODEMMLU = CodeMMLU.NAME\n",
    "CODEMMLU_TASK = CodeMMLU.Tasks.CODE_COMPLETION\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "FOR2WHILE = SyntacticMutations.FOR2WHILE\n",
    "FOR2ENUMERATE = SyntacticMutations.FOR2ENUMERATE\n",
    "\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "BOOLEAN_LITERAL = LogicalMutations.BOOLEAN_LITERAL\n",
    "DEMORGAN = LogicalMutations.DEMORGAN\n",
    "COMMUTATIVE_REORDER = LogicalMutations.COMMUTATIVE_REORDER\n",
    "CONSTANT_UNFOLD = LogicalMutations.CONSTANT_UNFOLD\n",
    "CONSTANT_UNFOLD_ADD = LogicalMutations.CONSTANT_UNFOLD_ADD\n",
    "CONSTANT_UNFOLD_MULT = LogicalMutations.CONSTANT_UNFOLD_MULT\n",
    "\n",
    "## Declaring Reasoning Model Name Constants\n",
    "GPT5 = ReasoningModels.GPT5['name']\n",
    "\n",
    "## Declaring Non-Reasoning Model Name Constants\n",
    "GPT4O = NonReasoningModels.GPT4O['name']\n",
    "CODESTRAL = NonReasoningModels.CODESTRAL['name']\n",
    "DEEPSEEK = NonReasoningModels.DEEPSEEK_CHAT['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_models = [getattr(ReasoningModels, model) for model in dir(ReasoningModels) if not model.startswith(\"_\")]\n",
    "non_reasoning_models = [getattr(NonReasoningModels, model) for model in dir(NonReasoningModels) if not model.startswith(\"_\")]\n",
    "print('Reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")\n",
    "print('=' * 50)\n",
    "print('Non-reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(non_reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = os.getenv(\"MONGODB_CODEMMLU_COLLECTION\")\n",
    "llmtester = LLMMCQInconsistencyTester(task_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = llmtester.question_database.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "`run_mcq_inconsistency_test` method is used for running MCQ inconsistency tests on MuCoCo.\n",
    "\n",
    "| Parameter              | Type        | Description                                                                                                              |\n",
    "| ---------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `prompt_helper`        | `str`       | String template for the appropriate prompt. Simply rename the `prompt_type` variable to `ONE_SHOT`, `FEW_SHOT` or `ZERO_SHOT` for `CodeMMLU` benchmark.\n",
    "| `output_file_path`     | `str`       | Full path to the CSV where predictions and metrics are saved. Filename is built from model, task type, and mutation tag. |\n",
    "| `num_tests`            | `int`       | Number of test questions to evaluate. Set to `num_tests` to run all tasks in mcq inconsistency.                                                                               |\n",
    "| `mutations`            | `List[str]` | Mutation operators to apply (e.g., `[\"FOR2WHILE\"]`, `[\"CONSTANT_UNFOLD\"]`). Empty list means **no_mutation**.            |\n",
    "| `model_name`           | `str`       | Identifier of the LLM under test (e.g., `GPT4O`). Used for routing and naming.                                           |\n",
    "| `task_set`      | `str`       | Only `CODEMMLU` for MCQ Inconsistency                                                           |                        | `take_type`      | `str`       | Only `CODEMMLU_TASK` for MCQ Inconsistency                                                           |\n",
    "| `continue_from_task`   | `str`       | Optional parameter for starting evaluation from a specified task ID corresponding to the task in MongoDB (e.g., `\"CodeMMLUMCQ15\"`)                                                 |\n",
    "\n",
    "The following example runs a MCQ Inconsistency test on the CodeMMLU benchmark for all tasks in CodeMMLU. To add mutations such as Random mutation, add the corresponding mutation string to the `mutations` list like so: `mutations = [RANDOM_MUTATION]`. The mutations available for MCQ inconsistency testing are declared as constants above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations=[]\n",
    "prompt_type = FEW_SHOT\n",
    "model_name = GPT4O\n",
    "task_type = CODEMMLU_TASK\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "\n",
    "results_dir =os.path.join(proj_dir, f'results/mcq_inconsistency/{model_name}')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "output_file_path=f\"{results_dir}/{task_set}_{prompt_type}_{mutation_str}.csv\"\n",
    "\n",
    "pass_count = llmtester.run_mcq_inconsistency_test(\n",
    "    prompt_helper= ReasoningMCQInconsistencyPromptTemplate().return_appropriate_prompt(prompt_type=prompt_type),\n",
    "    num_tests= num_tests,\n",
    "    prompt_type= prompt_type,\n",
    "    mutations=mutations,\n",
    "    output_file_path=output_file_path,\n",
    "    task_type =task_type,\n",
    "    task_set=CODEMMLU,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
