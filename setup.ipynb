{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Set Up for running experiments on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Step 1: Start by importing the .env file\n",
    "\n",
    "Ensure that you have the fields filled in \"MONGODB_URI\", \"GOOGLE_COLAB_HUGGINGFACE_TOKEN\", \"GITHUB_USERNAME\", \"GITHUB_BRANCH_NAME\" and \"GITHUB_PAT\" filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "73e1f2b2",
    "outputId": "8d6c8595-b035-4fdf-e1c6-f6070ee0dd6a"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Step 2: Install python-dotenv package and load the dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "767b326a",
    "outputId": "c227c27b-4f81-4f59-fae8-6610fb369997"
   },
   "outputs": [],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e04dab6b",
    "outputId": "504701d2-3f87-4df9-de9d-3662542670c5"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Step 3: Cloning the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f9c754f",
    "outputId": "93a48dcc-bdb5-4393-9f3f-9e5e4bc92501"
   },
   "outputs": [],
   "source": "# 1) Paste your GitHub PAT securely (no echo in output)\nimport os, subprocess\n\nGITHUB_USER = os.getenv('GITHUB_USERNAME')\nGITHUB_BRANCH_NAME = os.getenv(\"GITHUB_BRANCH_NAME\")\n\nGH_TOKEN = os.getenv(\"GITHUB_PAT\")\n\n# 2) Clone the specific branch (hide output so token isn't printed)\nurl = f\"https://{GITHUB_USER}:{GH_TOKEN}@github.com/your-org/your-repo.git\"\ncmd = [\"git\",\"clone\",\"-b\", GITHUB_BRANCH_NAME, \"--single-branch\", \"--depth\",\"1\", url]\nsubprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n# # 3) (Optional) Remove token from the saved remote to avoid accidental leaks\n# import pathlib, shlex, json\n# repo_dir = pathlib.Path(REPO)\n# subprocess.run([\"git\",\"-C\", str(repo_dir), \"remote\",\"set-url\",\"origin\",\n#                 f\"https://github.com/{GH_USER}/{REPO}.git\"], check=True)"
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Step 4: Change directory to the cloned Github Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1896038",
    "outputId": "22b46b95-56c3-4998-8542-4c39168c0964"
   },
   "outputs": [],
   "source": "%cd {\"your-repo\"}"
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Step 5: Pip install the necessary packages from requirements-colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c7177f",
    "outputId": "8a80d825-474f-4428-924c-c8979103ae26"
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements-colab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Step 6: Login into HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "5a09600c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv('GOOGLE_COLAB_HUGGINGFACE_TOKEN')\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Step 7: Downloading the desired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485,
     "referenced_widgets": [
      "9edec7641d3748f096dde329e3b7225b",
      "06297eeed6404eefb4ff60620caa6cd6",
      "81aed6e598124533b882fe956c90560d",
      "12f9c6d0671b468990a9c76224eb895a",
      "35f40985ab0541529a4f5cb8e48ec9a8",
      "8b0ea2c23701402196dbae4294fcea17",
      "6b3d792256ed44cb80d5406b33597b84",
      "42ac0a05fc3b4a308833a56e2eac01b8",
      "fe77d228c7d54e3c9ed5cf64e260e903",
      "a0cfba89a01d48808ed2e8557b7faa69",
      "3c67c7eff9b84aa9a1cb851296963c15",
      "5bbf257845e249d9b5c24e98c8af5f00",
      "aa99932fee2042e3b30faaffbf1184a1",
      "298f0ce9deb54b81bc6e886f6dacc0fc",
      "1e3c183acf3143f0a2896d0699ffdcd8",
      "cb8876a253814f3580630c52fdeb8860",
      "442deba0f5d04747a1b42164a494f8e7",
      "78240942327b4e8e9dac1a840c6edac6",
      "cb2c43405bc24c348955152fe6c695af",
      "badc82d960d445c9bff13e16448aa349",
      "09f90e426d524291bb81b840c44cb935",
      "f71b46b7ef6c43d79920fa4bad9a1a6c",
      "2eb1661902bb487a9ea9b6b109ec9d82",
      "2be3f4e25e894af4bf9601de0e90d00e",
      "f9f60f0f5c794ed9acd7097d04132d90",
      "15e09126a2154d5cb123886884473e79",
      "df89dd6e6d7749af9b97adcd9abd2f6a",
      "4cc650c6efee4eebb230617d3ef9e78a",
      "e027bf9c90914331aec5acea57140028",
      "163387a5c841480b8599e9a332965e61",
      "eac4577454004651a9f10883c425fb80",
      "05907e9ddbd54b73b4e002ba35d166d5",
      "04732a79240a46ad9d4e331d3fbb719d",
      "bf92fbcf19e74010b87a601bf286963b",
      "607f68b446bd41fa915be0066d19d096",
      "9760d4691dc7485a8604d05b83573ee4",
      "fe067b39c7184f98ae07075eb0bdb313",
      "f078d9de70cf42b1a2353356b77225b5",
      "810e452fa76c47ba87b5f50d4ee58013",
      "671445b6256848639a5e7a85a7d2ae19",
      "1a15f6a65e3a488890aad97577de0fe8",
      "ee954c416b914b1fb4ebe4d95ec30bb9",
      "7c4b7f18e00d44c0b54f7a3b6c0040c9",
      "2301777f18d64ce5b8e4f6a1e7227f4e",
      "4647649b15cb4539aeba19cc613ef343",
      "147970e9c90841728fade9dfdb5a2038",
      "34891e4e454d4dc28452a20a1989a9dd",
      "57ff24080dfd42acab4e2cded641ea54",
      "0f55b148e38f41298acc369747761596",
      "dc7045109f774245b7c1dc87bfd3f5f2",
      "0c96feb6e7024ffbb72c49efa84ad21c",
      "70dbfc8ff5b942f2a0aed8c53088aca3",
      "49a8bc6983534897955630827dbee7eb",
      "0324231b0c5d4546864b6bea9cf5ef7d",
      "fd7784095abc453fa98c5b2ec4e7df5a",
      "7e8371f618f848a5ac15ed07dc5a64b0",
      "4c87ffd3b1cd442ca3e59adccd42063c",
      "85f30abfb7f847d58a4e775d54baf02d",
      "1210df079b094328b355daf17e09a374",
      "c82ecb72294645709d949c661e94df3f",
      "83ae47edd7b8461ca536567b11e46f70",
      "37fc844fc06c4463b31904acfba69835",
      "84604c30c07b42e392712faa1decd97c",
      "ab50b6bac3d14d39b97b2a2a529a67ba",
      "b7b2b3d27e4d40aaa799b1888075614c",
      "8ac4e61301ea4e099962d3badb56ee6c",
      "8400a78343494484944d70d9eddc2bef",
      "e9e0520f74b74e3b920afb71cc5761b8",
      "696b836f030245758da10b754b5071e8",
      "9f690eb45d99422f9d7c96921e3f87c7",
      "dfeee387601c41e3827923e841298cbb",
      "a4eb1654bef34ee2a527b4815c128feb",
      "5f7279dea370457ca269dd0535171f5a",
      "67ae3ea72144438c8bf9be9be2bb0df4",
      "403b5d1399ef4eebab9b6a522a032509",
      "d19f95a3eab34dcc99e17c4158de510c",
      "a3e9864435624a7e9ccf3120ad2fe830",
      "38e4de7f9ec747dda40cf13bc25c8afa",
      "6c71384803ee4569b5e3554b04cc56d1",
      "d15b5abc15204dd3a832f6f02dabfbf5",
      "312b394aca454306b5e7a1ae0a751620",
      "4de155e8ca7742df8e93085e210a228e",
      "400f5f2687ba4426800fe8c173f59b35",
      "897f974d53c34346afbb75f7e0b8e249",
      "8880e8b3ab1c4bbd98787516801775c8",
      "b80d6fe302ed42a3b813fad0b5bcd42c",
      "5a360cd68d22495aada218ab8372f26c",
      "54411ae477794be2ad1d08bfa596378d",
      "977da37ddb694c7eb835872f074ca8f7",
      "e1f3e665e5e141b9b0c68ad9b9e90279",
      "c1fad951ed8f4e3a9b4ec643d18a4bb2",
      "ab504d89b10041438bad3cbc42b07707",
      "ab8474d9437646da9f54cc5b1f0aa603",
      "37bbf5f118b34a22842669beff6288ea",
      "f7fadb2f650e44cba4135f19bc639ecf",
      "8f7e92a051e0489da5ae80020ed30be1",
      "c58925edfc4c40c499e5f361d9f6cd23",
      "b70926b6d8344bb5909114ddba0287a1",
      "46efba2ef81f4a71b3eb30713e08d511",
      "673cf8002c7a49e8864c724dec456246",
      "222e908114174899a1327be7d4d60921",
      "984aa69fc3e8425fa58f015491904040",
      "f154c19f1dce4c058d794950b8f0cb9c",
      "579ab8eb98904455add09950d928bb8f",
      "fd182e791fc140c7bfde9b35ea77feda",
      "87525c5a15c34e2a928a7deba3ba36cf",
      "28f189d5305946d2bfe511faa7bc9d63",
      "9020ab7deeb449608ad98b938eec4acb",
      "73ab1a1747534c9dadd56c7adb5faaa8",
      "5a46388eaac04ef8a77c05d88a9fa347",
      "7137e82d9d7c4c9a99bf3a81212b1958",
      "7fb5a9c9697646c0ab07416459aa1b0c",
      "5ecedac456c04a44a6aece5fe3a68d0d",
      "378da81faa64427e901512c25b9659bc",
      "d414ce77f3ed4de48c4983074fdadd9a",
      "f7b0cfdffa0a492994067f6e8b6a63a8",
      "687b1ef80f4647d084ce22371dcbe061",
      "2408ccffbdb1443890e3444e0d99763f",
      "dd909faaa42e42c7aa785a8412361ae3",
      "93b4f52c213a47e0b976848b6424680e",
      "4a2ea34b8706462ca952bef93578352c",
      "b5eda16333a9483297f8e0a62c8d7704",
      "27fda8a7e2634193bad8c3998b49b35d",
      "4b368635e8b64a1586e3f6429cc4be9a",
      "01c7a96070a94e5ab8d5bcfa1cb6ad4e",
      "ddaa1154f8c04e109ea37e7666b45987",
      "ace4d0ed3dfb479aa199ef7229e548e1",
      "d8f03067853a487ca2b1a9c190395dfd",
      "ede1f93efda74e2dadd1cf961a144ecd",
      "dc2b772399bc40e18acaa5b7a1445060",
      "70cb97ae26c3462db5bbd9b8e7f85534",
      "76ebcad37f1b403cab0bd0b1ba571695"
     ]
    },
    "id": "2c5ed89c",
    "outputId": "dbd9f5bb-d7d3-45bd-fa9b-b3b53940a088"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Step 8: Ensuring that the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce1019ad",
    "outputId": "5ecde14d-d426-4115-eedd-8056fe65bfb5"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "# test prompt\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_text)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## FROM THIS STEP ON, COPY AND PASTE WHATEVER EXPERIMENT CELLS YOU NEED.\n",
    "\n",
    "Do remember to do this step first before uploading into Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Mutation Testing on Mistral LLM\n",
    "\n",
    "Using the code generation database from the MongoDB, this notebook will run **zero shot, one shot** and **few shot prompts** on a Mistral LLM. Each prompt technique also includes **no mutation, sequential mutated** and **random mutated** programs. In total, 9 experiments are run through this notebook. All logs are stored in csv files automatically for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_generation.code_generation_tester import CodeGenerationTester\n",
    "from llm_models.code_llms import Mistral\n",
    "from code_generation.prompt_templates.prompt_template import OpenEndedPromptTemplate\n",
    "from utility.constants import Tasks, PromptTypes, LexicalMutations, SyntacticMutations, LogicalMutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM = LexicalMutations.RANDOM\n",
    "SEQUENTIAL = LexicalMutations.SEQUENTIAL\n",
    "\n",
    "## Declaring Task Type Constants\n",
    "OUTPUT_PREDICTION = Tasks.OutputPrediction.NAME\n",
    "INPUT_PREDICTION = Tasks.InputPrediction.NAME\n",
    "\n",
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "FEW_SHOT = PromptTypes.FEW_SHOT\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "FOR2WHILE = SyntacticMutations.FOR2WHILE\n",
    "FOR2ENUMERATE = SyntacticMutations.FOR2ENUMERATE\n",
    "\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "BOOLEAN_LITERAL = LogicalMutations.BOOLEAN_LITERAL\n",
    "DEMORGAN = LogicalMutations.DEMORGAN\n",
    "COMMUTATIVE_REORDER = LogicalMutations.COMMUTATIVE_REORDER\n",
    "CONSTANT_UNFOLD = LogicalMutations.CONSTANT_UNFOLD\n",
    "CONSTANT_UNFOLD_ADD = LogicalMutations.CONSTANT_UNFOLD_ADD\n",
    "CONSTANT_UNFOLD_MULT = LogicalMutations.CONSTANT_UNFOLD_MULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = \"HumanEval\"\n",
    "\n",
    "try:\n",
    "    llmtester = CodeGenerationTester(f\"{task_set}_Code_Generation\")\n",
    "except Exception as e:\n",
    "    if task_set not in Tasks.CodeGeneration.BENCHMARKS:\n",
    "        raise ValueError(f\"An invalid task set was used. Only {Tasks.CodeGeneration.BENCHMARKS} are valid.\")\n",
    "    else:\n",
    "        print(f'llmtester could not launch due to the following error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = llmtester.question_database.count_documents({})\n",
    "llm = Mistral()\n",
    "model_name = \"mistral-small-2506\"\n",
    "mistral_results =os.path.join(proj_dir + '/results/code_generation/mistral')\n",
    "os.makedirs(mistral_results, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mutations = Tasks.CodeGeneration.MUTATIONS\n",
    "print(\"These are the valid mutation names:\")\n",
    "for idx, mutation in enumerate(valid_mutations):\n",
    "    print(idx, mutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Zero Shot Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations = []\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "prompt_type = ZERO_SHOT\n",
    "\n",
    "pass_count = llmtester.run_code_generation_test(\n",
    "    prompt_helper = OpenEndedPromptTemplate.zero_shot_prompt,\n",
    "    num_tests=num_tests,\n",
    "    # num_tests=10,\n",
    "    mutations = mutations,\n",
    "    prompt_type= prompt_type,\n",
    "    output_file_path=f\"{mistral_results}/{task_set}_{model_name}_{prompt_type}_{mutation_str}.csv\",\n",
    "    task_set = task_set,\n",
    ")\n",
    "\n",
    "print(f\"For no mutations, {pass_count} number of test cases passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations = [RANDOM_MUTATION]\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "prompt_type = ZERO_SHOT\n",
    "\n",
    "pass_count = llmtester.run_code_generation_test(\n",
    "    prompt_helper = OpenEndedPromptTemplate.zero_shot_prompt,\n",
    "    num_tests=num_tests,\n",
    "    # num_tests=10,\n",
    "    mutations = mutations,\n",
    "    prompt_type= prompt_type,\n",
    "    output_file_path=f\"{mistral_results}/{task_set}_{model_name}_{prompt_type}_{mutation_str}.csv\",\n",
    "    task_set = task_set,\n",
    ")\n",
    "\n",
    "print(f\"For no mutations, {pass_count} number of test cases passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations = [SEQUENTIAL_MUTATION]\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "prompt_type = ZERO_SHOT\n",
    "\n",
    "pass_count = llmtester.run_code_generation_test(\n",
    "    prompt_helper = OpenEndedPromptTemplate.zero_shot_prompt,\n",
    "    num_tests=num_tests,\n",
    "    # num_tests=10,\n",
    "    mutations = mutations,\n",
    "    prompt_type= prompt_type,\n",
    "    output_file_path=f\"{mistral_results}/{task_set}_{model_name}_{prompt_type}_{mutation_str}.csv\",\n",
    "    task_set = task_set,\n",
    ")\n",
    "\n",
    "print(f\"For no mutations, {pass_count} number of test cases passed.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}