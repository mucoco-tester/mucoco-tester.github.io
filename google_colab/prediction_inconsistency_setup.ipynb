{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "3389c645"
   },
   "source": [
    "# Set Up for running experiments on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "f223b665"
   },
   "source": [
    "#### Step 1: Start by importing the .env file\n",
    "\n",
    "Ensure that you have the fields filled in \"mongoDB_uri\", \"collab_token\", \"GITHUB_USERNAME\", \"GITHUB_BRANCH_NAME\" and \"GITHUB_PAT\" filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "73e1f2b2",
    "outputId": "82f58557-14b8-4769-e651-9bc9dff57b3a"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "45caaa9e"
   },
   "source": [
    "#### Step 2: Install python-dotenv package and load the dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "767b326a",
    "outputId": "084499cc-2466-44c9-b9d6-854e89040e4f"
   },
   "outputs": [],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e04dab6b",
    "outputId": "9256037a-0081-442d-b06b-9a9c1e5cd71f"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "9213c5c0"
   },
   "source": [
    "#### Step 3: Cloning the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f9c754f",
    "outputId": "641735d1-aa34-4e31-abca-6062b67ac796"
   },
   "outputs": [],
   "source": "# 1) Paste your GitHub PAT securely (no echo in output)\nimport os, subprocess\n\nGITHUB_USER = os.getenv('GITHUB_USERNAME')\nGITHUB_BRANCH_NAME = os.getenv(\"GITHUB_BRANCH_NAME\")\n\nos.environ[\"GH_TOKEN\"] = os.getenv(\"GITHUB_PAT\")\n\n# 2) Clone the specific branch (hide output so token isn't printed)\nurl = f\"https://{GITHUB_USER}:{os.environ['GH_TOKEN']}@github.com/your-org/your-repo.git\"\ncmd = [\"git\",\"clone\",\"-b\", GITHUB_BRANCH_NAME, \"--single-branch\", \"--depth\",\"1\", url]\nsubprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n# # 3) (Optional) Remove token from the saved remote to avoid accidental leaks\n# import pathlib, shlex, json\n# repo_dir = pathlib.Path(REPO)\n# subprocess.run([\"git\",\"-C\", str(repo_dir), \"remote\",\"set-url\",\"origin\",\n#                 f\"https://github.com/{GH_USER}/{REPO}.git\"], check=True)"
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "57fa6f5d"
   },
   "source": [
    "#### Step 4: Change directory to the cloned Github Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1896038",
    "outputId": "2734cbb2-0721-443b-ff69-8e244444a04c"
   },
   "outputs": [],
   "source": "%cd {\"your-repo\"}"
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "2d76a4d9"
   },
   "source": [
    "#### Step 5: Pip install the necessary packages from requirements-colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c7177f",
    "outputId": "f93bad2d-0205-43a5-8f95-9bc8748f98bc"
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements-colab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "04596637"
   },
   "source": [
    "#### Step 6: Login into HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "5a09600c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv('collab_token')\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "080aea75"
   },
   "source": [
    "#### Step 7: Downloading the desired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497,
     "referenced_widgets": [
      "f7ebc51975f64ea796aa6bb0f214d6f5",
      "fc902c5982154848bcc1dfa10d1fcded",
      "f6a10fa43bc34b688d19ca0798af3635",
      "42adc1ac0cef4abf968df0bff6e883c6",
      "c45c2721427c4860a7391727f336d051",
      "c14710986bf34fa2a7a82c87e1c6882f",
      "437d775e366d4ce7b09cdfccc4171363",
      "52ff27a9bdbd4083a58507ffbdaf05a7",
      "8d220e215e56447eacfcd33d8ae2cc07",
      "6720b5a5cbb04244a1e89eac174991ca",
      "d203d17c348445d69ccc4bffe7db09cc",
      "eac8b51851964528a8cbc300adbab132",
      "7ba2fc3b178a4ff686236a625e8074b6",
      "49b481c1f0814a8cb54d60b6e2651194",
      "6cb302f8a95b4738b14316930d243493",
      "50454f4c53504675b2457f4ff748b11f",
      "77c48eca1ca14cc0bdcb8144296018b2",
      "e153283c74314ea784c1d36fded738a0",
      "ff37afeccfeb46ada2f83ee240b1b81f",
      "7eadba8050554187b051166dd7b5f90e",
      "1dcbb537b1d44f1c8ccc8fcdaa63c4fc",
      "be327ef19aa4416bb1af9d3aebd3e5d5",
      "cd0d2c4143d744f3ab553c9de8e9d8d1",
      "7d2b023b409a468c9b41f507752bb227",
      "c32af0f814634ff888b456610e4ec6fd",
      "77b7b10d0155436d8a919ca07d32ee36",
      "ba60f4d7d8814d9896be33f45d96b715",
      "13934b352e6b42439238753f2c2a499d",
      "0689ba3172234b1a9045db980235fa80",
      "20c28187af974c6fbcc6c403a52b46cd",
      "f358adf27ec844d8b89714555ee5bde8",
      "703e347fc7614c06a2561046d5001a3b",
      "9f93bc01336f46329c79ee34ae378db3",
      "0d74cb148b5d423e8168fc1acc87f2b6",
      "feef2cb2b86040969e9c18433bba8dc1",
      "2682214eaf9f4d9c85eee1c272f5d833",
      "884472851bdd413d95adbeb05bff7f38",
      "e2fc6b9fe8e04f3b87bf636d5db08776",
      "9928549567294fbbae106e7c68ddbd5a",
      "ac598a32cf7c46aaacb3db25d181525f",
      "4e5700e1d8a743ff9124207139309d7d",
      "9378d0c604a14ee4bf4c23f83daad0a8",
      "b7b65db992154492b40c85eb58d99ecc",
      "d3c759e69ec94871bd04cd8bddd55cd8",
      "15b52f12d94947d09183a2df5f9a3fd0",
      "40d80cb7f3b74b848cc1013020094d8e",
      "4b0df9bc0f514e418eb643e08fdbcfa5",
      "023c9365ea6b4319a154f1bc4a1953df",
      "f1e438fb77214a20abd1a46c1b593ee8",
      "3c9269f8b6c3493da0aab12368e40905",
      "78be755e796c464e802bf29f5dbd5fe7",
      "fe453ebb79f4495bba96f70277fa4478",
      "484b6fc3550a4acfa0a438e91ccfa6cc",
      "f7e87b28ce2d441e88614e0354e98f08",
      "a1335486034441c08f9e3b5ae27d808f",
      "7959ba0d31204824ae1d9424a8fb1830",
      "7c90de4b8be24ba797b785b08bd998a4",
      "af50a55f48cf44bea3641eea98af92d7",
      "26ab397aa6954e678b3e4d7a4a8d575e",
      "b14824db068d4e60aa09311a6bf622bd",
      "c99d5f701d114e31a530e8f8a6bf221f",
      "4b6ec2b73e3d403582abcca4dc65b34c",
      "36e4e981077f42a2865da2ccea44b9b0",
      "ef85131bc6c94a7881002137e1cd2cf3",
      "b8f1ccccdabf4394b85e0b17422ececb",
      "396b2fe7b6fd47289b14c327e29c733d",
      "fe3309b082e34c46a08b0377db620880",
      "6a578f040b81443e9bdb0212b54dc817",
      "69e864457f2d4b30aceb755447e76089",
      "f42cb9236c6e4ac0acefa25acbf92e25",
      "6b5d8ada6beb43fb9e94351398c408a6",
      "5d16daa412a14b7390e87a7a23a5f934",
      "905b67221d1a452ca7c51bbf2bcf94c3",
      "0ccc07c73bd34ea89f9ed0b1cda6a1ec",
      "df08435ed8e84565a33dedc14f57f152",
      "260f6a88af24455696e9d7c1b2131046",
      "cb1f29a4c58d4a3199ec02d7585931b0",
      "58652ce1796d470ab3cd6ab1d1c3beb8",
      "77bdba43fe7b4af2bd0d158abfc3a612",
      "a8a3d36351134d8080a3df1f79e98270",
      "955d3c220325467db6cc9046bbbd7db5",
      "bee066df80b3476b94492b62c04ea7d5",
      "92b7ca697d5046d6905e61855f9fc0f6",
      "1b4cbe069a024c3789d284b7e223cc05",
      "e3d67c122a74423bb1901755ca609573",
      "ba87f087d12646a5bca76341d83ff254",
      "a907695663374468b1f36405aa4a5cf1",
      "67b0e451bc37474b983c0598516f47ca",
      "b77a749e75d14ef69c4aba869e467054",
      "2a1a1f474189472b81fd1b8b4fc49258",
      "f7cff4dac35c44e98de7be186996e84b",
      "069606533d094bd4b705131e112063ec",
      "a58478627fe64574a0165c112b6e8495",
      "c73ee9e9d0c740c8816b590420872af0",
      "4bb9d065a740405f961777ac21feeb2b",
      "5971bf3433744ce3b94463a492af92e1",
      "4569320842e8463dbc25394d90d6a933",
      "388ba2ebdf5d42b08d398644b8845b6e",
      "4c8514bbe4ba49279c095d5c352954ac",
      "f3c34ae5e9404fee95235edf338a2683",
      "3eb0badad25a46e58c2d9c7753a28731",
      "2cf451f2e8b24ca786650479b6f738d8",
      "81bdb37b5bf34b59a1a734572b7d11d5",
      "a553659b043b4d6fba5d68c7a48c38d5",
      "980c371d18ff49bc887b2e6fb857e1d6",
      "25998a4801ac4a46b16294323abd54ee",
      "5cd7a68de29c4163a84f8dcb0ab99e51",
      "6f2e7db4fb1e4cd6afca0296da75118f",
      "757e3944fc444e2e80d3f3aa7c1ada60",
      "54bc484d7d0747efb6ce1ee41e2ef7eb",
      "040184e9a5684a259e0be3c6fa03cdc7",
      "02cf6b9217b74cf7b8b6deb51d24dddd",
      "2f2f5b4f4029496aa346091152a33e7e",
      "dc2f0974fc7742b2948a3b93c9854fc5",
      "998c8f2853c74668a49936f80e63cc4d",
      "a8f7c7684eff4e4d82acdd3eb5ee903b",
      "96ab19369f2f453b8765761aa61979ef",
      "4fb8d44bacd44a2ab53b1428bee9c8a1",
      "e652396581754515bc38b5e958b73e8b",
      "954851e3726f452f8f57bdeeb34ba657",
      "5341847a712345ab92535781312d4fc4",
      "1ea9054f5be4433f8cd39200d06f492c",
      "65c50944481f4552b7306a7b47381697",
      "a346f4a0f2b84551923bbfe8ef81120f",
      "845bd46216e24d57b378b2d06937751d",
      "2bf60887a4a34f359d052181251db325",
      "f35d3a8d42f14269b494b950d418333b",
      "d0f3862faaad43bbb8b9040ea08085b8",
      "8c6036b0a3054633898b21156bd08352",
      "80cff22719db464691b1535c50f36c15",
      "a6d06df78f6d42f78195f61bc2dd4eaf",
      "8fbeff085c464fbe855ef8a4de6d771a",
      "76ee4fb88c734c0db6eceba4efe04119",
      "d7b7355dee0d4eadbec607135196635b",
      "f6b812153be247cc920035ac13910ed2",
      "a9903c54833d4d09b05dc265a8785c0c",
      "7cd081e9b75342409e3ca228d59d42d6",
      "73c3bc4eb62e4b46b9be38059546884b",
      "33dc8ab4e84a462a9c76d77b99a3fd8b",
      "fb96d2f461e54342aa4877b1f28f9005",
      "1d4d5fb6a32f401c9929899325b5bebd",
      "0330d59ddb474637b85cc8550276564d",
      "e80e5627253040ed80dc06642fe6e0d4",
      "60b7bef5d892427facf88f768683a56e",
      "5984b2c4ad4e4129af5a59323adb357d",
      "6da7c40ee0534b1c97225fd46ccfa05a",
      "d790373ab89e4b86b010ffd644a66045",
      "11aae474e6b843f6ba00f0ecda1ea2cc",
      "0278cc73652e4f05961d028eab5ff4cc",
      "ed5a45ca346f4020a73529436f2d3471",
      "755c2b7339794bd0bd334b8a311c5a96",
      "bf46e3f68bcf458e9b548fdff880f7e7",
      "b0220d0f67c24243adcc1bb1bd3dbc82",
      "339b3b413d7240edab4592ca96c3f480",
      "4c8818fa12a440d4be59d7a65708585b",
      "63c7f016215e4b779671af15b16f694e",
      "a0e6fd122fb8482b8beaa00a76a66ddb",
      "649382167a8a4817b3a5d7a8fc8ec9ed",
      "ec6b70e633d34102957b7f61c186d123",
      "df34a04455bb4cefa3fbe7272439fbd7",
      "168958729da9486d9d0bb876f1b0cc0d",
      "c474586cb6024249850d53ae2fbf0bbf",
      "ba91d0777a2240dfa186d1015c08bddc",
      "4b3c75f1f50748a39475f383e758e4f4",
      "3815924d0e9f41a7a839833b5f24618a"
     ]
    },
    "id": "2c5ed89c",
    "outputId": "ece09ae4-6c60-43b3-ec06-8f3e7be913e4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "6343e390"
   },
   "source": [
    "#### Step 8: Ensuring that the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8aKn4Y2LX-m",
    "outputId": "96ea07da-b5d6-4895-e5b8-98b1d0cff520"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "# test prompt\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_text)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "bd21a2f5"
   },
   "source": [
    "## FROM THIS STEP ON, COPY AND PASTE WHATEVER EXPERIMENT CELLS YOU NEED.\n",
    "\n",
    "Do remember to do this step first before uploading into Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "370e65c5"
   },
   "source": [
    "## LLM Consistency Testing with Mistral LLM\n",
    "\n",
    "This notebook contains code for testing code inconsistency in Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "9eff811c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "847501b0"
   },
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "60fab327"
   },
   "outputs": [],
   "source": [
    "from prediction_inconsistency.prediction_inconsistency_tester import LLMConsistencyTester\n",
    "from prediction_inconsistency.prompt_templates.prompt_template import PredictionInconsistencyPromptTemplate\n",
    "from utility.constants import Tasks, PromptTypes, LexicalMutations, SyntacticMutations, LogicalMutations, ReasoningModels, NonReasoningModels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "9cc57730"
   },
   "source": [
    "# Declaring constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "76fd1f92"
   },
   "outputs": [],
   "source": [
    "## Declaring Task Type Constants\n",
    "OUTPUT_PREDICTION = Tasks.OutputPrediction.NAME\n",
    "INPUT_PREDICTION = Tasks.InputPrediction.NAME\n",
    "\n",
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "FEW_SHOT = PromptTypes.FEW_SHOT\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "FOR2WHILE = SyntacticMutations.FOR2WHILE\n",
    "FOR2ENUMERATE = SyntacticMutations.FOR2ENUMERATE\n",
    "\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "BOOLEAN_LITERAL = LogicalMutations.BOOLEAN_LITERAL\n",
    "DEMORGAN = LogicalMutations.DEMORGAN\n",
    "COMMUTATIVE_REORDER = LogicalMutations.COMMUTATIVE_REORDER\n",
    "CONSTANT_UNFOLD = LogicalMutations.CONSTANT_UNFOLD\n",
    "CONSTANT_UNFOLD_ADD = LogicalMutations.CONSTANT_UNFOLD_ADD\n",
    "CONSTANT_UNFOLD_MULT = LogicalMutations.CONSTANT_UNFOLD_MULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnTjaleVgEeq",
    "outputId": "a4baec6a-158e-464b-8dd9-42f926fb39b2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "284305e21d324b7abd1be67b12a58746",
      "31f676f0df8f472b93702416301ab420",
      "c42fe9b6b8ad4acb8dae493380afb15f",
      "d2fa4979c0ea4c98a7a7ab4b7ce27ed3",
      "2ac34f8575684534bde59508e3f2c3d0",
      "17d8042e30bc4c0aa830df8179840b81",
      "4d7a216f274d45828cdf7c6f996cac6d",
      "a4f2da32b40a4a67a3707f7e5e3a63d1",
      "7827ff616db7483ca7bd90a2f6a8fe58",
      "8fe90cdde07c4f10a1abcb34e8ce4ae8",
      "7b62b8cae07d4731997cdd1089c604e6"
     ]
    },
    "id": "6p7dvxaJppve",
    "outputId": "c9c096c5-d4b0-448e-fb90-9ba5c42bb19e"
   },
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount(\"/content/drive\")\n\nimport os\nimport shutil\nimport torch\nimport gc\n\nprompt_type = FEW_SHOT\nmodel_name = \"google/gemma-3-12b-it\"\ntask_type = OUTPUT_PREDICTION\ntask_set = \"HumanEval\"\ndatabase_name = f\"{task_set}_Input_Output\"\nllmtester = LLMConsistencyTester(database_name, n=5)\n\nresults_base_dir = os.path.join(proj_dir, f\"results\", task_type, model_name, prompt_type)\nos.makedirs(results_base_dir, exist_ok=True)\n\nmutation_configs = [\n    #[],\n    #[RANDOM_MUTATION],\n    #[SEQUENTIAL_MUTATION],\n    #[FOR2WHILE],\n    [FOR2ENUMERATE],\n    #[DEMORGAN],\n    #[LITERAL_FORMAT],\n    #[BOOLEAN_LITERAL],\n    #[COMMUTATIVE_REORDER],\n    #[CONSTANT_UNFOLD],\n    #[CONSTANT_UNFOLD_ADD],\n    #[CONSTANT_UNFOLD_MULT]\n]\n\nfor mutations in mutation_configs:\n    mutation_str = \"_\".join(mutations) if mutations else \"no_mutation\"\n\n    output_file_path = os.path.join(results_base_dir, f\"{task_set}_{prompt_type}_{mutation_str}\")\n    drive_dst_dir = os.path.join(f\"/content/drive/MyDrive/your-repo/\", task_type, model_name)\n    drive_dst = os.path.join(drive_dst_dir, f\"{task_set}_{prompt_type}_{mutation_str}\")\n\n    os.makedirs(drive_dst_dir, exist_ok=True)\n\n    # Clear GPU memory before running\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    try:\n        # Run experiment\n        llmtester.run_code_consistency_test(\n            prompt_helper=PredictionInconsistencyPromptTemplate.return_model_appropriate_prompt(\n                task_type, prompt_type, model_name\n            ),\n            num_tests=llmtester.question_database.count_documents({}),\n            prompt_type=prompt_type,\n            mutations=mutations,\n            output_file_path=output_file_path,\n            task_set=task_set,\n            task_type=task_type,\n            model_name=model_name,\n        )\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(f\"OOM error for {mutation_str}, skipping this run\")\n            torch.cuda.empty_cache()\n            gc.collect()\n            continue\n        else:\n            raise e\n\n    # Only copy if file exists\n    if os.path.exists(output_file_path):\n        shutil.copy(output_file_path, drive_dst)\n        print(f\"Saved {mutation_str} results to Drive\")\n    else:\n        print(f\"Output file not found for {mutation_str}, skipping Drive copy\")\n\n    # Extra cleanup after each run\n    torch.cuda.empty_cache()\n    gc.collect()"
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "_-ZqQn8VgvbS"
   },
   "source": [
    "## LLM Consistency Testing with Mistral LLM\n",
    "\n",
    "This notebook contains code for testing code inconsistency in Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "CmgqSZgWgwYf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "E8PoFMCag1EX"
   },
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "R2Ex_iGIg23y"
   },
   "outputs": [],
   "source": [
    "from mcq_inconsistency.mcq_inconsistency_tester import LLMMCQInconsistencyTester\n",
    "from mcq_inconsistency.prompt_templates.prompt_template import MCQInconsistencyPromptTemplate\n",
    "from utility.constants import CodeMMLU, LexicalMutations, SyntacticMutations, LogicalMutations, PromptTypes, ReasoningModels, NonReasoningModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "9XlzTQu4g44c"
   },
   "outputs": [],
   "source": [
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "FEW_SHOT = PromptTypes.FEW_SHOT\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "FOR2WHILE = SyntacticMutations.FOR2WHILE\n",
    "FOR2ENUMERATE = SyntacticMutations.FOR2ENUMERATE\n",
    "\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "BOOLEAN_LITERAL = LogicalMutations.BOOLEAN_LITERAL\n",
    "DEMORGAN = LogicalMutations.DEMORGAN\n",
    "COMMUTATIVE_REORDER = LogicalMutations.COMMUTATIVE_REORDER\n",
    "CONSTANT_UNFOLD = LogicalMutations.CONSTANT_UNFOLD\n",
    "CONSTANT_UNFOLD_ADD = LogicalMutations.CONSTANT_UNFOLD_ADD\n",
    "CONSTANT_UNFOLD_MULT = LogicalMutations.CONSTANT_UNFOLD_MULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rk8xGI7dg8hi",
    "outputId": "fdc35a1d-b949-4611-a462-181079d2dfa7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory before running the test\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check available memory\n",
    "print(f\"Available GPU memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvnzqgGAg8l6",
    "outputId": "8c57cb70-b4f2-4a59-f79c-37d93961af4e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "41ab1b0be456451f8c0b03961c8fe668",
      "f5001f77f17d4be0ad570e0167882353",
      "be4f85a4d4bb422b961df9ce4d8bbb36",
      "55148f2040154618802f7104c332d83b",
      "142c0352718a414f9706a7340e321579",
      "456d758fc14e4a7d940b938ccfbc781a",
      "504484d7d48247ef944f899f9c4db25c",
      "0cc97c052fad453cab5d932d76c23a64",
      "a231dcdd382b46fca728835642943d9b",
      "4cabcf3d202040459f885130fd5682c2",
      "ab5fa66c63b2428cafc8c92cd13d1130",
      "783c31addadf49279e169d80ff507c78",
      "ed52015316dc45cea1be47a2576188f0",
      "5e895b83d44c42c681dc027bdd1727bc",
      "6ed08c26d1f34b9c98a4c8dc3d54e37d",
      "660fe7d3f6594ed0addfb0caa9e77244",
      "59a22b39545143d3a723209ac6767296",
      "903eb06210a9407da83eeb4cc89731c3",
      "f5f3ad93a3a04216b836d9344677800f",
      "f06cd79c4f054580afa73fe3f3301d87",
      "f31f2853b80a47299341de4b94714030",
      "035109840edf4111b5fad367595e0fdd",
      "180d7f50fbaf4dcb80ff3c176fd3593a",
      "5a9cfb05b9834cbf9384fe1facc5294e",
      "8841267d445d4ca39bfa4bdf4a6b3071",
      "e469961673ce4bb684ffffb6c51e01c2",
      "b0e065e2182e45ba966f14659e952a1c",
      "e0734006cc6549ceb60719d97c3a0543",
      "181e53feeec54edf9ad9c88bf81a6cba",
      "2bcf5c24323f401b87863d09378dbcd3",
      "60995f00257f44b6b29f80c6a32336f5",
      "f553f56b353b42dab29ed9638a2a6496",
      "d7e1462bf2b04701a0c54c8125a3e62f",
      "b6b0e72b595f40b6b59d7bccfbe748f6",
      "f851301fb1374aaabea8513de7afd633",
      "c0eee4b22b0d456992cc043218d3be2a",
      "164dfa9d30e94692b85dfd5b2ca4a169",
      "c45ef86623b34588b2f58432e4b45d64",
      "f2379aae299840da8f10a4a9d0ed52d4",
      "877b4e8cba8245daa7d581dfe9e1472f",
      "f2b640277bbd43229948f284b6b0d459",
      "f753d73504d545fe91bce32f6e1cf6ff",
      "590cde15778244118eb43bd55eae5554",
      "84ae8ca58685412bbcc8f95cf496f358",
      "27afdcea04a4465a987641097862bf42",
      "59559a54a5184a82b07571e2bab3b2ca",
      "ea9cc9400b5247ca992cd051ecf2f1b1",
      "58bca56d2b8042e6b13505e710884c6b",
      "01d6a504913545c787967c2819835b6b",
      "72e2d37f9de44b0b8203f1d6e35f3407",
      "8f3eb7f670f24b0ca38dae8322c6468e",
      "e9ea740a5a4f460480c81139598036db",
      "32dd0143bcc648efbaae6ac49cd493d2",
      "83c57a4c290a4262be39e0c92847caaa",
      "48538970dac74218b5cd95e4ea96fe1f",
      "8a6b1a5a0f0c48d490003ec9bd03631a",
      "86d5e5cacab64a38ad5c2b437d396962",
      "a47e8b9ce5ec4b4e96d61b8ec9859481",
      "53a33ac8d2fd4a2f8a44c5ce15282251",
      "536abd74a7b741cfb0f2bff0add914aa",
      "9f872e43a30d441cab5482eb9d6a04db",
      "420836cb1cf14d4fa1619a635f45ecdc",
      "339d46a1342842fab83f9aef5263abd3",
      "e72738d8ee794ef693b94b8f47edd2b9",
      "bb67124d0809455b855c457159a023b1",
      "f2c9d676f2394ad3b59fa54b8859dcf4",
      "83e094bcc3c841e89b111cf4b6023e23",
      "7962015f476a4b1f81fe1708ffcc37d2",
      "94e8953acb734ba7aadacb0a5e9f0e38",
      "7f2502602bcc4611b87b229c27b0054b",
      "e911b4a3de6b4cf095ad4984b17240ef",
      "71d6d47a4a064a20960ab9c7d908dd4f",
      "a8b4b5c77708432299def6a1a67ca379",
      "b599ae4ca64e4a08a554de13c3fdc880",
      "02b27f873c904742a4e5c812225e85a9",
      "4d4645704fc54707a174a13243b24903",
      "5a6ef6cbca89430da87eb40361bff5a2",
      "3c690a57419c4f629bd152c296fb4403",
      "dfd87eacaae6481097df73b1934250a4",
      "be40c01d3bcc4f39a04acde92dd25b67",
      "c2ab91fa03574871851f600040efb3ce",
      "c48dd0a15f6b41d0a7788ec6a98dc5cb",
      "b3fe7043a8714f91bc378b48520a08c7",
      "48887323b5224f59ae10349390fe0405",
      "de812a21710e4df298c11b4e47815d7a",
      "152fb6825c9047e9826385ce5ed2e893",
      "b7597817c08a4b73a094df810a2b59e6",
      "32484ee49cdc4eaeabaf14c9486d13c6",
      "53194e2c30c848008fa345829ece1ea0",
      "50b8297bede449debf8e782002dc60b6",
      "581be322dc7b4917a782b7b605fffad1",
      "b3500408f11c451483893b5019383bfb",
      "1bf7c268626e41e297d37e5f74fbf6eb",
      "088b25e0832e434a862dc3abb1a77d12",
      "1474af55a074455e9792d55ffb163f07",
      "8d419f77f00d4e73a613b430ec3625b1",
      "0ede9af836f048acbfc406efb2c00c89",
      "c17c15d8265d4f4f9d59754f67794b6b",
      "5f89a8f5613e46bb9c575d68ea117da2",
      "38888c6be2f04056b172964181a8bb14",
      "87e08cb82ab64ba6ac7eef2ddd182c98",
      "fa1d4083c16341439c327b89bc21e0e9",
      "09c764e782f74fd4aa04ac72e9308460",
      "c4be9dde36814ba7a611052a65b9a09f",
      "22b9e9c0eb914b55b36a78792c2dab7f",
      "c421f0c4a99e443586aec0378e690212",
      "dca7796325d54c3ca095d9c9ca345bc5",
      "fb9f54b6500e421ea34f753f441869a9",
      "d0b2f67e2ee04720b7370fec30490602",
      "3950dde649eb45a8ae39b6c5bc598b24",
      "c2501cbe4aa74715b2c7b764ab20cef6",
      "1fe173c034894b3080151880c5556d6c",
      "7cfa2efee0c2470baf269eada9ef1820",
      "2869927875124f77b204552f85bef514",
      "16c64a3e30884a688cf70cb94d7872fe",
      "1569a0ca3e5b452e90baf1ca4c7dbfec",
      "42cda61419bd4c56b009559024d4ff18",
      "42fc6a11232c40fcae93168bdf4e697f",
      "ca3207ce026a4cc9a4f9c5acfb96e5ae",
      "4d15ef24d57f4370aa5c3ae0fcb05829",
      "32e6bd6d8a6f490fb65e3830de0aec51",
      "d5f97ab88e9b47c8814416b92f11dc44",
      "fa3683f6c3b048079ba1f112c0991448",
      "15e04300373548f580b81d454643af2a",
      "85dd9178f7cb4a74b256f8be1914112f",
      "36e4a22fc6824fb686cd0f020871a580",
      "922a36e402104aef8ea893dcde31f472",
      "a4101ac3241a4f3bbdd0e769a40b3882",
      "2536a5516fd6431a851498e70fd136b1",
      "2d063af6b6934ee980fb1d4ff6fa03df",
      "093fe1c1b41f4158be034a5d3d3baf70",
      "ca751a433d554ea6b61df495eb5b2f10"
     ]
    },
    "id": "dDuTd6yJhCqG",
    "outputId": "21e0a300-124e-434e-efac-20907d1412e2"
   },
   "outputs": [],
   "source": "import os\nimport shutil\nimport subprocess\nimport torch\nimport gc\n\ntask_set = \"CodeMMLU_MCQ_code_completion\"\nllmtester = LLMMCQInconsistencyTester(task_set)\n\nprompt_type = FEW_SHOT\nmodel_name = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n\ntask_type = CodeMMLU.Tasks.CODE_COMPLETION\n\nresults_base_dir = os.path.join(proj_dir, f\"results\", task_type, model_name)\nos.makedirs(results_base_dir, exist_ok=True)\n\nmutation_configs = [\n    [],\n    [RANDOM_MUTATION],\n    [SEQUENTIAL_MUTATION],\n    [FOR2WHILE],\n    [FOR2ENUMERATE],\n    [DEMORGAN],\n    [LITERAL_FORMAT],\n    [BOOLEAN_LITERAL],\n    [COMMUTATIVE_REORDER],\n    [CONSTANT_UNFOLD],\n    [CONSTANT_UNFOLD_ADD],\n    [CONSTANT_UNFOLD_MULT]\n]\n\nfor mutations in mutation_configs:\n    mutation_str = \"_\".join(mutations) if mutations else \"no_mutation\"\n\n    output_file_path = os.path.join(results_base_dir, f\"Gemma_MCQ35_{task_set}_{prompt_type}_{mutation_str}.csv\")\n    drive_dst_dir = os.path.join(f\"/content/drive/MyDrive/your-repo/\", task_type, model_name)\n    drive_dst = os.path.join(drive_dst_dir, f\"{task_set}_{prompt_type}_{mutation_str}.csv\")\n\n    # Ensure Drive folder exists\n    os.makedirs(drive_dst_dir, exist_ok=True)\n\n    # Clear GPU memory before running each experiment\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    try:\n        # Run experiment\n        llmtester.run_mcq_inconsistency_test(\n        prompt_helper= MCQInconsistencyPromptTemplate.return_model_appropriate_prompt(prompt_type, model_name),\n        #num_tests=llmtester.question_database.count_documents({}),\n        num_tests=1,\n        prompt_type= prompt_type,\n        mutations=mutations,\n        output_file_path=output_file_path,\n        task_type =task_type,\n        task_set=\"CodeMMLU\",\n        model_name=model_name,\n        continue_from_task=\"CodeMMLUMCQ35\",\n        )\n\n        # Overwrite results on Drive\n        shutil.copy(output_file_path, drive_dst)\n        print(f\"Saved {mutation_str} results to Drive\")\n\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(f\"OOM error for {mutation_str}, skipping this run\")\n            torch.cuda.empty_cache()\n            gc.collect()\n            continue\n        else:\n            raise e\n    except FileNotFoundError as e:\n        print(f\"File not found for {mutation_str}: {e}\")\n        continue\n    except Exception as e:\n        print(f\"An unexpected error occurred for {mutation_str}: {e}\")\n        continue"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}