{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MuCoCo RQ3 Experiment Results Aggregation for Model Confidence\n",
    "\n",
    "This notebook is used to aggregate the results for MuCoCo RQ3 experiments. The results should be stored in MuCoCo_results/MuCoCo_experiment_results/model_output_confidence in the project root folder. The final aggregated results from this notebook are used in tables XI (mpact of varying mode confidence on Inconsistency and Accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "proj_dir = os.path.abspath(os.path.join(current_dir, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_confidence(model_folder_dir: str, model_name: str, task: str):\n",
    "    csv_files = [file for file in os.listdir(model_folder_dir) if file.endswith('.csv')]\n",
    "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "    benchmarks = [\"HumanEval\", \"CruxEval\", \"CodeMMLU\"]\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        benchmark_files = [file for file in csv_files if benchmark in file and 'ensemble' not in file]\n",
    "\n",
    "        for file in benchmark_files:\n",
    "            file_path = os.path.join(model_folder_dir, file)\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # Make a copy of the base columns\n",
    "            benchmark_df = data.loc[:, ['task_id', 'geometric', 'failure_type']].copy()\n",
    "\n",
    "            # For each threshold, create a new column for classified failure type\n",
    "            for threshold in thresholds:\n",
    "                new_col = f'failure_type_{threshold}'\n",
    "                classified_values = []\n",
    "\n",
    "                for conf, failure in zip(data['geometric'], data['failure_type']):\n",
    "                    if isinstance(failure, float) or (isinstance(failure, str) and \"AssertionError\" in failure and \"Mutation\" not in failure):\n",
    "                        if conf >= threshold:\n",
    "                            classified_values.append(failure)  # correct (no error)\n",
    "                        else:\n",
    "                            classified_values.append(\"Confidence_lower_than_threshold\") \n",
    "                    else:\n",
    "                        classified_values.append(failure)\n",
    "\n",
    "                benchmark_df[new_col] = classified_values\n",
    "\n",
    "            # Save as new CSV\n",
    "            out_path = os.path.join(proj_dir, f\"MuCoCo_experiment_results/model_output_confidence/{task}/{model_name}\")\n",
    "            os.makedirs(name = out_path, exist_ok=True)\n",
    "            benchmark_df.to_csv(os.path.join(out_path,f\"{file.split('.csv')[0]}_confidence.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_res_dir = os.path.join(proj_dir, \"MuCoCo_experiment_results/\")\n",
    "\n",
    "\n",
    "for task in [\"input_prediction\", \"mcq_inconsistency\"]:\n",
    "    target_dir = os.path.join(main_res_dir, task)\n",
    "    for file in os.listdir(target_dir):\n",
    "        if any(m.lower() in file.lower() for m in [\"Gemma\", \"Qwen\", \"Llama\"]):\n",
    "            model_folder_path = os.path.join(target_dir, file)\n",
    "            check_confidence(model_folder_path, file, task)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_two_df(df1: pd.DataFrame, df2: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    common_ids = set(df1[\"task_id\"]) & set(df2[\"task_id\"])\n",
    "    if not common_ids:\n",
    "        print(\"⚠️ No matching task_ids found between the two DataFrames.\")\n",
    "        return df1.iloc[0:0], df2.iloc[0:0]  # return empty aligned frames\n",
    "\n",
    "    df1_filtered = df1[df1[\"task_id\"].isin(common_ids)].copy()\n",
    "    df2_filtered = df2[df2[\"task_id\"].isin(common_ids)].copy()\n",
    "\n",
    "    df1_filtered = df1_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "    df2_filtered = df2_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "\n",
    "    df1_filtered = df1_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "    df2_filtered = df2_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "\n",
    "    return df1_filtered, df2_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dir = os.path.join(main_res_dir, \"model_output_confidence\")\n",
    "thresholds = [\"\", 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "res_df = pd.DataFrame()\n",
    "acc_df = pd.DataFrame()\n",
    "inconsistency_dict = {}\n",
    "accuracy_dict = {}\n",
    "\n",
    "def check_valid_failure(failure: str):\n",
    "    if isinstance(failure, float) or (isinstance(failure, str) and \"AssertionError\" in failure and \"Mutation\" not in failure):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for task in ['input_prediction', 'mcq_inconsistency']:\n",
    "    res_dir = os.path.join(conf_dir, task)\n",
    "    for models in os.listdir(res_dir):\n",
    "        model_dict = inconsistency_dict.get(models, {})\n",
    "        model_acc_dict = accuracy_dict.get(models, {})\n",
    "        if models == \".DS_Store\":\n",
    "            continue\n",
    "        model_res_path = os.path.join(res_dir, models)\n",
    "        for benchmark in [\"HumanEval\", \"CodeMMLU\", \"CruxEval\"]:\n",
    "            csv_logs = [log for log in os.listdir(model_res_path) if benchmark in log and log.endswith('.csv')]\n",
    "            if len(csv_logs) == 0:\n",
    "                continue\n",
    "\n",
    "            no_mut = [log for log in csv_logs if \"no_mutation\" in log][0]\n",
    "            no_mut_data = pd.read_csv(os.path.join(model_res_path, no_mut))\n",
    "            csv_logs.pop(csv_logs.index(no_mut))\n",
    "\n",
    "            # iterating csv logs through each threshold\n",
    "            for threshold in thresholds:\n",
    "                total_comparisons = 0\n",
    "                total_inconsistencies = 0\n",
    "\n",
    "                total_answered = 0\n",
    "                total_correct = 0\n",
    "\n",
    "                if threshold != \"\":\n",
    "                    col_name = f\"failure_type_{threshold}\"\n",
    "                else: \n",
    "                    col_name = f\"failure_type\"\n",
    "\n",
    "                no_mut_failure_type = no_mut_data.loc[:, col_name]\n",
    "\n",
    "                for no_mut_failure in no_mut_failure_type:\n",
    "                    if not check_valid_failure(no_mut_failure):\n",
    "                        continue\n",
    "                    elif isinstance(no_mut_failure, float):\n",
    "                        total_correct += 1\n",
    "                        sub_dict2 = model_acc_dict.get(threshold, {})\n",
    "                        sub_dict2['correct'] = sub_dict2.get('correct',0) + 1\n",
    "                        model_acc_dict[threshold] = sub_dict2\n",
    "                    total_answered += 1\n",
    "                    sub_dict2 = model_acc_dict.get(threshold, {})\n",
    "                    sub_dict2['answered'] = sub_dict2.get('answered',0) + 1\n",
    "                    model_acc_dict[threshold] = sub_dict2\n",
    "            \n",
    "                for mut_log in sorted(csv_logs):\n",
    "                    mut_data = pd.read_csv(os.path.join(model_res_path, mut_log))\n",
    "\n",
    "                    mut_failure_type = mut_data.loc[:, col_name]\n",
    "                    count = 0\n",
    "                    count2 = 0\n",
    "\n",
    "                    no_mut_temp = no_mut_data.copy()\n",
    "                    mut_temp = mut_data.copy()\n",
    "\n",
    "                    no_mut_temp, mut_temp = standardize_two_df(no_mut_temp, mut_temp)\n",
    "\n",
    "\n",
    "                    for no_mut_failure , mut_failure in zip(no_mut_temp.loc[:, col_name], mut_temp.loc[:, col_name]):\n",
    "\n",
    "                        if not check_valid_failure(mut_failure) or not check_valid_failure(no_mut_failure):\n",
    "                            continue \n",
    "                        \n",
    "                        total_answered += 1\n",
    "\n",
    "                        sub_dict2 = model_acc_dict.get(threshold, {})\n",
    "                        sub_dict2['answered'] = sub_dict2.get('answered',0) + 1\n",
    "                        model_acc_dict[threshold] = sub_dict2\n",
    "                        \n",
    "                        if isinstance(mut_failure, float):\n",
    "                            total_correct += 1\n",
    "                            sub_dict2 = model_acc_dict.get(threshold, {})\n",
    "                            sub_dict2['correct'] = sub_dict2.get('correct',0) + 1\n",
    "                            model_acc_dict[threshold] = sub_dict2\n",
    "\n",
    "                        if not all(check_valid_failure(failure) for failure in [no_mut_failure, mut_failure]) and (\"AssertionError\" in str(no_mut_failure) and \"AssertionError\" in str(mut_failure)):\n",
    "                            continue \n",
    "\n",
    "                        if (isinstance(no_mut_failure, float) or isinstance(mut_failure, float)):\n",
    "                            total_comparisons += 1\n",
    "                            sub_dict = model_dict.get(threshold, {})\n",
    "                            sub_dict['comparisons'] = sub_dict.get('comparisons',0) + 1\n",
    "                            model_dict[threshold] = sub_dict\n",
    "\n",
    "                        if (\n",
    "                            isinstance(no_mut_failure, float) and \n",
    "                            isinstance(mut_failure, str) \n",
    "                            ) or (\n",
    "                            isinstance(mut_failure, float) and \n",
    "                            isinstance(no_mut_failure, str)  \n",
    "                        ):\n",
    "                            if 'qwen' in models.lower() and benchmark == \"CodeMMLU\" and threshold ==\"\":\n",
    "                                # print(count2-1, no_mut_failure, mut_failure)\n",
    "                                count += 1\n",
    "\n",
    "                            total_inconsistencies += 1\n",
    "                            sub_dict = model_dict.get(threshold, {})\n",
    "                            sub_dict['inconsistencies'] = sub_dict.get('inconsistencies',0) + 1\n",
    "                            model_dict[threshold] = sub_dict\n",
    "\n",
    "                try:\n",
    "                    res_df.loc[f\"{threshold}\", f\"{models}_{benchmark}_{task}\"] = f\"{total_inconsistencies}/{total_comparisons} = {round(total_inconsistencies*100/total_comparisons, 2)}\"\n",
    "                except ZeroDivisionError:\n",
    "                    res_df.loc[f\"{threshold}\", f\"{models}_{benchmark}_{task}\"] = f\"{total_inconsistencies}/{total_comparisons} = 0\"\n",
    "                try:\n",
    "                    acc_df.loc[f\"{threshold}\", f\"{models}_{benchmark}_{task}\"] = f\"{total_correct}/{total_answered} = {round(total_correct*100/total_answered, 2)}\"\n",
    "                except ZeroDivisionError:\n",
    "                    res_df.loc[f\"{threshold}\", f\"{models}_{benchmark}_{task}\"] = f\"{total_correct}/{total_answered} = 0\"\n",
    "            \n",
    "\n",
    "        \n",
    "        inconsistency_dict[models] = model_dict\n",
    "        accuracy_dict[models] = model_acc_dict\n",
    "\n",
    "\n",
    "new_df_index = [\n",
    "    \"gemma-3-12b-it_HumanEval_input_prediction\",\n",
    "    \"gemma-3-12b-it_CruxEval_input_prediction\",\n",
    "    \"gemma-3-12b-it_CodeMMLU_mcq_inconsistency\",\n",
    "    \"Qwen2.5-Coder-14B-Instruct_HumanEval_input_prediction\",\n",
    "    \"Qwen2.5-Coder-14B-Instruct_CruxEval_input_prediction\",\n",
    "    \"Qwen2.5-Coder-14B-Instruct_CodeMMLU_mcq_inconsistency\",\n",
    "    \"LLama-3.1-8B_HumanEval_input_prediction\",\n",
    "    \"LLama-3.1-8B_CruxEval_input_prediction\",\n",
    "    \"LLama-3.1-8B_CodeMMLU_mcq_inconsistency\"\n",
    "]\n",
    "\n",
    "\n",
    "res_df = res_df.reindex(new_df_index, axis=1)\n",
    "acc_df = acc_df.reindex(new_df_index, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def merge_by_confidence(data):\n",
    "    merged = defaultdict(lambda: {\"inconsistencies\": 0, \"comparisons\": 0})\n",
    "    \n",
    "    for model, conf_dict in data.items():\n",
    "        for conf, stats in conf_dict.items():\n",
    "            merged[conf][\"inconsistencies\"] += stats.get(\"inconsistencies\", 0)\n",
    "            merged[conf][\"comparisons\"] += stats.get(\"comparisons\", 0)\n",
    "    \n",
    "    return dict(merged)\n",
    "\n",
    "# Example usage\n",
    "merged_data1 = merge_by_confidence(inconsistency_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Inconsistency of models (gemma, qwen, llama) at each confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold, res_dict in merged_data1.items():\n",
    "    inconsistencies = res_dict['inconsistencies']\n",
    "    comparisons = res_dict['comparisons']\n",
    "\n",
    "    res_df.loc[f\"{threshold}\", f\"Aggregated\"] = f\"{inconsistencies}/{comparisons} = {round(inconsistencies*100/comparisons, 2)}\"\n",
    "\n",
    "print(res_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Accuracy of models (gemma, qwen, llama) at each confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_confidence(data):\n",
    "    merged = defaultdict(lambda: {\"answered\": 0, \"correct\": 0})\n",
    "    \n",
    "    for model, conf_dict in data.items():\n",
    "        for conf, stats in conf_dict.items():\n",
    "            merged[conf][\"answered\"] += stats.get(\"answered\", 0)\n",
    "            merged[conf][\"correct\"] += stats.get(\"correct\", 0)\n",
    "    \n",
    "    return dict(merged)\n",
    "\n",
    "\n",
    "merged_data2 = merge_by_confidence(accuracy_dict)\n",
    "\n",
    "\n",
    "for threshold, res_dict in merged_data2.items():\n",
    "    answered = res_dict['answered']\n",
    "    correct = res_dict['correct']\n",
    "    # print(answered)\n",
    "\n",
    "    acc_df.loc[f\"{threshold}\", f\"Aggregated\"] = f\"{correct}/{answered} = {round(correct*100/answered, 2)}\"\n",
    "\n",
    "print(acc_df.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
