{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MuCoCo RQ4 Second Order Results Aggregation\n",
    "\n",
    "This notebook is used to aggregate the results for MuCoCo second order experiments. The experiment results are stored in MuCoCo_results/MuCoCo_experiment_results/second_order from the project directory. The results are reflected in Table XII (MUCOCO’s Scalability to multiple (2) mutations vs. atomic mutations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.data_log_functions import DataLogHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_two_df(df1: pd.DataFrame, df2: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    common_ids = set(df1[\"task_id\"]) & set(df2[\"task_id\"])\n",
    "    if not common_ids:\n",
    "        print(\"⚠️ No matching task_ids found between the two DataFrames.\")\n",
    "        return df1.iloc[0:0], df2.iloc[0:0]  # return empty aligned frames\n",
    "\n",
    "    df1_filtered = df1[df1[\"task_id\"].isin(common_ids)].copy()\n",
    "    df2_filtered = df2[df2[\"task_id\"].isin(common_ids)].copy()\n",
    "\n",
    "    df1_filtered = df1_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "    df2_filtered = df2_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "\n",
    "    df1_filtered = df1_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "    df2_filtered = df2_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "\n",
    "    return df1_filtered, df2_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_code_generation_logs(res_dir: str, filter: Tuple[str] = (), anti_filter: Tuple[str] = ()):\n",
    "    \n",
    "    if filter is None:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (os.path.isfile(os.path.join(res_dir, f)) and f.endswith(\".csv\"))]\n",
    "    else:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (\n",
    "            os.path.isfile(os.path.join(res_dir, f)) and \n",
    "            f.endswith(\".csv\") and \n",
    "            all(sub in f for sub in filter)) and\n",
    "            all(sub not in f for sub in anti_filter)\n",
    "            ]\n",
    "\n",
    "    log_file_names = [csv_file_name.replace('.csv', '') for csv_file_name in csv_logs]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=log_file_names, index = log_file_names)\n",
    "    for file_name in log_file_names:\n",
    "        results_df.loc[file_name, file_name] = float('nan')\n",
    "\n",
    "    while len(csv_logs) > 0:\n",
    "        log1_file_name = csv_logs.pop()\n",
    "        for log2_file_name in csv_logs:\n",
    "            log1_file_path = os.path.join(res_dir, log1_file_name)\n",
    "            log2_file_path = os.path.join(res_dir, log2_file_name)\n",
    "\n",
    "            log1 = pd.read_csv(log1_file_path)\n",
    "            log2 = pd.read_csv(log2_file_path) \n",
    "            print(log1_file_name, log2_file_name)\n",
    "            log1, log2 = standardize_two_df(log1, log2)\n",
    "            log1_inconsistencies, log2_inconsistencies = DataLogHelper.compare_code_generation_dataframe_results(log1=log1, log2=log2)\n",
    "\n",
    "            results_df.loc[log1_file_name.replace('.csv', ''), log2_file_name.replace('.csv', '')] = log1_inconsistencies\n",
    "            results_df.loc[log2_file_name.replace('.csv', ''), log1_file_name.replace('.csv', '')] = log2_inconsistencies\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_csv_name(file_name: str)-> str:\n",
    "    mutation_type = file_name.split(\"shot_\")[-1]\n",
    "    if \"_\" in mutation_type:\n",
    "        mutation = mutation_type.replace(\"_\", \" \").title()\n",
    "        return mutation\n",
    "    return mutation_type.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_category(log_name:str) -> str | None:\n",
    "\n",
    "    second_order_mutation = {\n",
    "        \"Second Order\": [\n",
    "            \"for2while_random\",\n",
    "            \"for2while_constant_unfold\",\n",
    "            \"constant_unfold_random\"\n",
    "        ]    \n",
    "    }\n",
    "\n",
    "    for cat, mut in second_order_mutation.items():\n",
    "        for m in mut:\n",
    "            if m in log_name:\n",
    "                return cat\n",
    "        \n",
    "    else:\n",
    "        return \"Atomic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_logs_against_no_mutation(res_dir: str, filter: Tuple[str] = (), anti_filter: Tuple[str] = ()):\n",
    "    \n",
    "    if filter is None:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (os.path.isfile(os.path.join(res_dir, f)) and f.endswith(\".csv\"))]\n",
    "    else:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (\n",
    "            os.path.isfile(os.path.join(res_dir, f)) and \n",
    "            f.endswith(\".csv\") and \n",
    "            all(sub in f for sub in filter)) and\n",
    "            all(sub not in f for sub in anti_filter)\n",
    "            ]\n",
    "\n",
    "    csv_logs.sort()\n",
    "    target_log_name = [l for l in csv_logs if \"no_mutation\" in l][-1]\n",
    "    csv_logs.pop(csv_logs.index(target_log_name))\n",
    "    target_log_path = os.path.join(res_dir, target_log_name)\n",
    "    target_log = pd.read_csv(target_log_path)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    total_inconsistencies = 0\n",
    "    total_questions = 0\n",
    "    total_success = 0\n",
    "    total_answered = 0\n",
    "\n",
    "    category_dict = {}\n",
    "    mutation_dict = {}\n",
    "\n",
    "    for log_name in csv_logs:\n",
    "        # print(log_name)\n",
    "\n",
    "        log_category = obtain_category(log_name)\n",
    "                \n",
    "        log2_file_path = os.path.join(res_dir, log_name)\n",
    "        log2 = pd.read_csv(log2_file_path) \n",
    "\n",
    "        inconsistency_dict = DataLogHelper.compare_code_generation_dataframe_results(log1=target_log, log2=log2)\n",
    "\n",
    "        # Adding results into the dataframe\n",
    "        cleaned_mutation_name = clean_up_csv_name(log_name.replace('.csv', ''))\n",
    "        results_df.loc[cleaned_mutation_name, \"Inconsistency Score\"] = f\"{inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']}/{inconsistency_dict['total_inconsistency_questions']} ({round((inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies'])*100/inconsistency_dict['total_inconsistency_questions'], 2)}%)\"\n",
    "\n",
    "        if 'No Mutation' in results_df.index:\n",
    "            pass\n",
    "        else:\n",
    "            results_df.loc['No Mutation', \"Inconsistency Score\"] = \"N/A\"\n",
    "            results_df.loc['No Mutation', \"Model Accuracy\"] = f\"{(inconsistency_dict['log1_success'])}/{inconsistency_dict['log1_total_answered']} ({round((inconsistency_dict['log1_success'])*100/inconsistency_dict['log1_total_answered'], 2)}%)\"\n",
    "        results_df.loc[cleaned_mutation_name, \"Model Accuracy\"] = f\"{(inconsistency_dict['log2_success'])}/{inconsistency_dict['log2_total_answered']} ({round((inconsistency_dict['log2_success'])*100/inconsistency_dict['log2_total_answered'], 2)}%)\"\n",
    "        \n",
    "        if total_success == 0:\n",
    "            total_success += inconsistency_dict['log1_success']\n",
    "        \n",
    "        if total_answered == 0:\n",
    "            total_answered += inconsistency_dict['log1_total_answered']\n",
    "\n",
    "        if 'model_ensemble' in log_name.lower() or \"ensemble\" not in log_name.lower() :\n",
    "            total_inconsistencies += inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']\n",
    "            total_questions += inconsistency_dict['total_inconsistency_questions']\n",
    "            total_success += inconsistency_dict['log2_success']\n",
    "            total_answered += inconsistency_dict['log2_total_answered']\n",
    "\n",
    "        \n",
    "        if log_category:\n",
    "            d: Dict = category_dict.get(log_category, {})\n",
    "            d['total_inconsistencies'] = d.get('total_inconsistencies', 0) + inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']\n",
    "            d['total_questions'] = d.get('total_questions', 0) + inconsistency_dict['total_inconsistency_questions']\n",
    "            d['total_success'] = d.get('total_success', 0) + inconsistency_dict['log2_success']\n",
    "            d['total_answered'] = d.get('total_answered', 0) + inconsistency_dict['log2_total_answered']\n",
    "            category_dict[log_category] = d\n",
    "\n",
    "\n",
    "        # adding results in mutation_dict, with the mutation name as key\n",
    "        mutation_dict[cleaned_mutation_name] = {\n",
    "            'total_inconsistencies': inconsistency_dict['log1_inconsistencies']+ inconsistency_dict['log2_inconsistencies'],\n",
    "            'total_questions': inconsistency_dict['total_inconsistency_questions'],\n",
    "            'total_success': inconsistency_dict['log2_success'],\n",
    "            'total_answered': inconsistency_dict['log2_total_answered']\n",
    "        }\n",
    "    \n",
    "    results_df = pd.concat([\n",
    "        results_df[results_df.index.str.lower().str.contains(\"no mutation\")],\n",
    "\n",
    "        results_df[\n",
    "            ~results_df.index.str.lower().str.contains(\"ensemble\") &\n",
    "            ~results_df.index.str.lower().str.contains(\"no mutation\")\n",
    "        ],\n",
    "\n",
    "        results_df[results_df.index.str.lower().str.contains(\"ensemble\")]\n",
    "    ])\n",
    "\n",
    "    ## Adding aggregated second order results and atomic results\n",
    "    for key, mut_dict in category_dict.items():\n",
    "        mut_inconsistencies = mut_dict['total_inconsistencies']\n",
    "        mut_questions = mut_dict['total_questions']\n",
    "        mut_success = mut_dict['total_success']\n",
    "        mut_answered = mut_dict['total_answered']\n",
    "        results_df.loc[f\"{key} Results\", \"Inconsistency Score\"] = f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "        results_df.loc[f\"{key} Results\", \"Model Accuracy\"] = f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)}%)\"\n",
    "\n",
    "        mutation_dict[f\"{key} Results\"] = mut_dict\n",
    "\n",
    "\n",
    "    results_df.loc[\"Aggregated Results\", \"Inconsistency Score\"] = f\"{total_inconsistencies}/{total_questions} ({round(total_inconsistencies*100/total_questions, 2)})\"\n",
    "    results_df.loc[\"Aggregated Results\", \"Model Accuracy\"] = f\"{total_success}/{total_answered} ({round(total_success*100/total_answered, 2)}%)\"\n",
    "\n",
    "    return [\n",
    "        results_df, \n",
    "        category_dict, \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "proj_dir = os.path.abspath(os.path.join(current_dir, \"..\",))\n",
    "\n",
    "def obtain_benchmark_task_csv(benchmark: str, task: str) -> pd.DataFrame:\n",
    "\n",
    "    final_df = pd.DataFrame()  # start with an empty DataFrame\n",
    "\n",
    "    res_dir = os.path.join(proj_dir, f\"MuCoCo_experiment_results/second_order/{task}/\")\n",
    "    try:\n",
    "        res, category_dict = compare_logs_against_no_mutation(res_dir=res_dir, filter=(benchmark, ))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{res_dir} does not exist.\")\n",
    "\n",
    "    res_df = pd.DataFrame(res)\n",
    "\n",
    "    if final_df.empty:\n",
    "        final_df = res_df\n",
    "    else:\n",
    "        final_df = pd.concat([final_df, res_df], axis=1)\n",
    "\n",
    "    # final_df.to_csv(\"combined_results.csv\", index=True, header=True)\n",
    "    return final_df, category_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "tasks = {\n",
    "    'input_prediction': ['HumanEval', \"CruxEval\"],\n",
    "    'output_prediction': ['HumanEval', \"CruxEval\"],\n",
    "    'mcq_inconsistency': ['CodeMMLU'],\n",
    "}\n",
    "\n",
    "task_dict = {}\n",
    "overall_dict = {}\n",
    "all_benchmark_dict = {}\n",
    "dfs = []\n",
    "\n",
    "def combine_two_dictionaries(d1: dict, d2: dict) -> dict:\n",
    "    # make it PURE (return a new merged dict)\n",
    "    out = copy.deepcopy(d1)\n",
    "    for k, inner2 in d2.items():\n",
    "        if k not in out:\n",
    "            out[k] = copy.deepcopy(inner2)             \n",
    "        else:\n",
    "            out[k] += inner2\n",
    "    return out\n",
    "\n",
    "task_df = pd.DataFrame()\n",
    "\n",
    "for task, benchmarks in tqdm(tasks.items()):\n",
    "    # Dictionary for storing results to aggregate by task\n",
    "    task_d = {}\n",
    "\n",
    "    print(f\"Aggregating for {task} logs\")\n",
    "    for benchmark in benchmarks:\n",
    "\n",
    "        print(f\"Working on {benchmark} now...\")\n",
    "        final_df, aggregated_dict = obtain_benchmark_task_csv(benchmark, task)\n",
    "\n",
    "        final_df = final_df.rename(columns={\n",
    "            \"Inconsistency Score\": f\"{task}_{benchmark}_inconsistency\",\n",
    "            \"Model Accuracy\": f\"{task}_{benchmark}_accuracy\"\n",
    "        })\n",
    "\n",
    "        if task_df.empty:\n",
    "            task_df = final_df\n",
    "        else:\n",
    "            task_df = pd.concat([task_df, final_df], axis=1)\n",
    "\n",
    "        benchmark_dict = {}\n",
    "\n",
    "        for mut_cat, mut_cat_dict in aggregated_dict.items():\n",
    "            # make a NEW dict here instead of aliasing res_dir\n",
    "            if not benchmark_dict.get(mut_cat, None):\n",
    "                benchmark_dict[mut_cat] = mut_cat_dict.copy()\n",
    "            else:\n",
    "                for key, val in mut_cat_dict.items():\n",
    "                    benchmark_dict[mut_cat][key] += val\n",
    "\n",
    "        d1 = task_d.get(task, {})\n",
    "\n",
    "        if not d1:\n",
    "            task_d[task] = copy.deepcopy(mut_cat_dict)\n",
    "        else:\n",
    "            task_d[task] = combine_two_dictionaries(d1, mut_cat_dict)    \n",
    "    \n",
    "    task_dict[task] = task_d[task]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# MuCoCo Results Aggregated Across Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "order = [\n",
    "    \"No Mutation\", \n",
    "    \"Random\",\n",
    "    \"Constant Unfold\",\n",
    "    \"For2while\",\n",
    "    \"Constant Unfold Random\",\n",
    "    \"For2While Random\",\n",
    "    \"For2While Constant Unfold\",\n",
    "    \"Atomic Results\",\n",
    "    \"Second Order Results\",\n",
    "    \"Aggregated Results\"\n",
    "]\n",
    "\n",
    "task_df = task_df.reindex(order)\n",
    "\n",
    "\n",
    "print(task_df.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
