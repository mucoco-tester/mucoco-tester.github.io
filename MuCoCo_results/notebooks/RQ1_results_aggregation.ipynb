{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MuCoCo RQ 1 Experiment Results Aggregation\n",
    "\n",
    "This notebook is used to aggregate the results for MuCoCo RQ1 experiments. The results are stored in MuCoCo_results/MuCoCo_experiment_results/ in the project root folder. The final aggregated results from this notebook are used in tables VI (aggregating across model), VII (aggregating across tasks) and VIII (aggregating across benchmarks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.data_log_functions import DataLogHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_two_df(df1: pd.DataFrame, df2: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    common_ids = set(df1[\"task_id\"]) & set(df2[\"task_id\"])\n",
    "    if not common_ids:\n",
    "        print(\"⚠️ No matching task_ids found between the two DataFrames.\")\n",
    "        return df1.iloc[0:0], df2.iloc[0:0]  # return empty aligned frames\n",
    "\n",
    "    df1_filtered = df1[df1[\"task_id\"].isin(common_ids)].copy()\n",
    "    df2_filtered = df2[df2[\"task_id\"].isin(common_ids)].copy()\n",
    "\n",
    "    df1_filtered = df1_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "    df2_filtered = df2_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "\n",
    "    df1_filtered = df1_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "    df2_filtered = df2_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "\n",
    "    return df1_filtered, df2_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_code_generation_logs(res_dir: str, filter: Tuple[str] = (), anti_filter: Tuple[str] = ()):\n",
    "    \n",
    "    if filter is None:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (os.path.isfile(os.path.join(res_dir, f)) and f.endswith(\".csv\"))]\n",
    "    else:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (\n",
    "            os.path.isfile(os.path.join(res_dir, f)) and \n",
    "            f.endswith(\".csv\") and \n",
    "            all(sub in f for sub in filter)) and\n",
    "            all(sub not in f for sub in anti_filter)\n",
    "            ]\n",
    "\n",
    "    log_file_names = [csv_file_name.replace('.csv', '') for csv_file_name in csv_logs]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=log_file_names, index = log_file_names)\n",
    "    for file_name in log_file_names:\n",
    "        results_df.loc[file_name, file_name] = float('nan')\n",
    "\n",
    "    while len(csv_logs) > 0:\n",
    "        log1_file_name = csv_logs.pop()\n",
    "        for log2_file_name in csv_logs:\n",
    "            log1_file_path = os.path.join(res_dir, log1_file_name)\n",
    "            log2_file_path = os.path.join(res_dir, log2_file_name)\n",
    "\n",
    "            log1 = pd.read_csv(log1_file_path)\n",
    "            log2 = pd.read_csv(log2_file_path) \n",
    "            log1, log2 = standardize_two_df(log1, log2)\n",
    "            log1_inconsistencies, log2_inconsistencies = DataLogHelper.compare_code_generation_dataframe_results(log1=log1, log2=log2)\n",
    "\n",
    "            results_df.loc[log1_file_name.replace('.csv', ''), log2_file_name.replace('.csv', '')] = log1_inconsistencies\n",
    "            results_df.loc[log2_file_name.replace('.csv', ''), log1_file_name.replace('.csv', '')] = log2_inconsistencies\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_csv_name(file_name: str)-> str:\n",
    "    mutation_type = file_name.split(\"shot_\")[-1]\n",
    "    if \"_\" in mutation_type:\n",
    "        mutation = mutation_type.replace(\"_\", \" \").title()\n",
    "        return mutation\n",
    "    return mutation_type.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_category(log_name:str) -> str | None:\n",
    "    mutation_categories = {\n",
    "        \"Lexical\": [\n",
    "            \"literal_format\",\n",
    "            \"random\",\n",
    "            \"sequential\"\n",
    "        ],\n",
    "        \"Syntactic\": [\n",
    "            \"for2while\",\n",
    "            \"for2enumerate\"\n",
    "        ],\n",
    "        \"Logical\": [\n",
    "            \"boolean_literal\",\n",
    "            \"constant_unfold\",\n",
    "            \"constant_unfold_add\",\n",
    "            \"constant_unfold_mult\",\n",
    "            \"demorgan\",\n",
    "            \"commutative_reorder\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for cat, mut in mutation_categories.items():\n",
    "        for m in mut:\n",
    "            if m in log_name:\n",
    "                return cat\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_logs_against_no_mutation(res_dir: str, filter: Tuple[str] = (), anti_filter: Tuple[str] = ()):\n",
    "    \n",
    "    if filter is None:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (os.path.isfile(os.path.join(res_dir, f)) and f.endswith(\".csv\"))]\n",
    "    else:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (\n",
    "            os.path.isfile(os.path.join(res_dir, f)) and \n",
    "            f.endswith(\".csv\") and \n",
    "            all(sub in f for sub in filter)) and\n",
    "            all(sub not in f for sub in anti_filter)\n",
    "            ]\n",
    "\n",
    "    csv_logs.sort()\n",
    "    target_log_name = [l for l in csv_logs if \"no_mutation\" in l][-1]\n",
    "    csv_logs.pop(csv_logs.index(target_log_name))\n",
    "    target_log_path = os.path.join(res_dir, target_log_name)\n",
    "    target_log = pd.read_csv(target_log_path)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    total_inconsistencies = 0\n",
    "    total_questions = 0\n",
    "    total_success = 0\n",
    "    total_answered = 0\n",
    "\n",
    "    category_dict = {}\n",
    "    mutation_dict = {}\n",
    "\n",
    "    for log_name in csv_logs:\n",
    "        # print(log_name)\n",
    "\n",
    "        log_category = obtain_category(log_name)\n",
    "                \n",
    "        log2_file_path = os.path.join(res_dir, log_name)\n",
    "        log2 = pd.read_csv(log2_file_path) \n",
    "\n",
    "        target_log, log2 = standardize_two_df(target_log, log2)\n",
    "\n",
    "        inconsistency_dict = DataLogHelper.compare_code_generation_dataframe_results(log1=target_log, log2=log2)\n",
    "\n",
    "        # Adding results into the dataframe\n",
    "        cleaned_mutation_name = clean_up_csv_name(log_name.replace('.csv', ''))\n",
    "        results_df.loc[cleaned_mutation_name, \"Inconsistency Score\"] = f\"{inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']}/{inconsistency_dict['total_inconsistency_questions']} ({round((inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies'])*100/inconsistency_dict['total_inconsistency_questions'], 2)}%)\"\n",
    "        results_df.loc['No Mutation', \"Inconsistency Score\"] = \"N/A\"\n",
    "        results_df.loc['No Mutation', \"Model Accuracy\"] = f\"{(inconsistency_dict['log1_success'])}/{inconsistency_dict['log1_total_answered']} ({round((inconsistency_dict['log1_success'])*100/inconsistency_dict['log1_total_answered'], 2)}%)\"\n",
    "        results_df.loc[cleaned_mutation_name, \"Model Accuracy\"] = f\"{(inconsistency_dict['log2_success'])}/{inconsistency_dict['log2_total_answered']} ({round((inconsistency_dict['log2_success'])*100/inconsistency_dict['log2_total_answered'], 2)}%)\"\n",
    "\n",
    "        if total_success == 0:\n",
    "            total_success += inconsistency_dict['log1_success']\n",
    "        \n",
    "        if total_answered == 0:\n",
    "            total_answered += inconsistency_dict['log1_total_answered']\n",
    "\n",
    "        if 'model_ensemble' in log_name.lower() or \"ensemble\" not in log_name.lower() :\n",
    "            total_inconsistencies += inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']\n",
    "            total_questions += inconsistency_dict['total_inconsistency_questions']\n",
    "            total_success += inconsistency_dict['log2_success']\n",
    "            total_answered += inconsistency_dict['log2_total_answered']\n",
    "\n",
    "        \n",
    "        if log_category:\n",
    "            d: Dict = category_dict.get(log_category, {})\n",
    "            d['total_inconsistencies'] = d.get('total_inconsistencies', 0) + inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']\n",
    "            d['total_questions'] = d.get('total_questions', 0) + inconsistency_dict['total_inconsistency_questions']\n",
    "            d['total_success'] = d.get('total_success', 0) + inconsistency_dict['log2_success']\n",
    "            d['total_answered'] = d.get('total_answered', 0) + inconsistency_dict['log2_total_answered']\n",
    "            category_dict[log_category] = d\n",
    "\n",
    "        # adding results in mutation_dict, with the mutation name as key\n",
    "        mutation_dict[cleaned_mutation_name] = {\n",
    "            'total_inconsistencies': inconsistency_dict['log1_inconsistencies']+ inconsistency_dict['log2_inconsistencies'],\n",
    "            'total_questions': inconsistency_dict['total_inconsistency_questions'],\n",
    "            'total_success': inconsistency_dict['log2_success'],\n",
    "            'total_answered': inconsistency_dict['log2_total_answered']\n",
    "        }\n",
    "    \n",
    "    results_df = pd.concat([\n",
    "        results_df[results_df.index.str.lower().str.contains(\"no mutation\")],\n",
    "\n",
    "        results_df[\n",
    "            ~results_df.index.str.lower().str.contains(\"ensemble\") &\n",
    "            ~results_df.index.str.lower().str.contains(\"no mutation\")\n",
    "        ],\n",
    "\n",
    "        results_df[results_df.index.str.lower().str.contains(\"ensemble\")]\n",
    "    ])\n",
    "\n",
    "    ## Adding aggregated second order results and atomic results\n",
    "    for key, mut_dict in category_dict.items():\n",
    "        mut_inconsistencies = mut_dict['total_inconsistencies']\n",
    "        mut_questions = mut_dict['total_questions']\n",
    "        mut_success = mut_dict['total_success']\n",
    "        mut_answered = mut_dict['total_answered']\n",
    "        results_df.loc[f\"{key} Results\", \"Inconsistency Score\"] = f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "        results_df.loc[f\"{key} Results\", \"Model Accuracy\"] = f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)}%)\"\n",
    "\n",
    "        mutation_dict[f\"{key} Results\"] = mut_dict\n",
    "\n",
    "\n",
    "    results_df.loc[\"Aggregated Results\", \"Inconsistency Score\"] = f\"{total_inconsistencies}/{total_questions} ({round(total_inconsistencies*100/total_questions, 2)})\"\n",
    "    results_df.loc[\"Aggregated Results\", \"Model Accuracy\"] = f\"{total_success}/{total_answered} ({round(total_success*100/total_answered, 2)}%)\"\n",
    "\n",
    "\n",
    "    return [\n",
    "        results_df, \n",
    "        category_dict, \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"Qwen2.5-Coder-14B-Instruct\" : \"Qwen2.5-Coder-14B-Instruct\",\n",
    "    \"gemma-3-12b-it\": \"Gemma-3-12b-it\",\n",
    "    \"deepseek-reasoner\": \"DeepSeek-V3.2-Exp (Non-thinking Mode)\",\n",
    "    \"LLama-3.1-8B\": \"LLama-3.1-8B\",\n",
    "    \"gpt-5\" : \"GPT-5\",\n",
    "    \"gpt-4o\": \"GPT-4o\",\n",
    "    \"codestral-latest\": \"codestral-2508\",\n",
    "}\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "proj_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "def obtain_benchmark_task_csv(benchmark: str, task: str) -> pd.DataFrame:\n",
    "\n",
    "    final_df = pd.DataFrame()  # start with an empty DataFrame\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterating through each model in model_dict\n",
    "    for k, m in model_dict.items():\n",
    "        # print(k)\n",
    "        res_dir = os.path.join(proj_dir, f\"MuCoCo_experiment_results/{task}/{k}\")\n",
    "        try:\n",
    "            res, category_dict = compare_logs_against_no_mutation(res_dir=res_dir, filter=(benchmark, ))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{res_dir} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        res_df = pd.DataFrame(res)\n",
    "\n",
    "        res_df = res_df.add_prefix(f\"{m} \")\n",
    "\n",
    "        if final_df.empty:\n",
    "            final_df = res_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df, res_df], axis=1)\n",
    "\n",
    "        final_dict[m] = category_dict\n",
    "\n",
    "    return final_df, final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "tasks = {\n",
    "    'mcq_inconsistency': ['CodeMMLU'],\n",
    "    'input_prediction': ['HumanEval', \"CruxEval\"],\n",
    "    'output_prediction': ['HumanEval', \"CruxEval\"],\n",
    "    'code_generation': ['BigCodeBench', \"HumanEval\"],\n",
    "}\n",
    "\n",
    "task_dict = {}\n",
    "overall_dict = {}\n",
    "all_benchmark_dict = {}\n",
    "dfs = []\n",
    "\n",
    "def combine_two_dictionaries(d1: dict, d2: dict) -> dict:\n",
    "    out = copy.deepcopy(d1)\n",
    "    for k, inner2 in d2.items():\n",
    "        if k not in out:\n",
    "            out[k] = copy.deepcopy(inner2)              \n",
    "        else:\n",
    "            for kk, vv in inner2.items():\n",
    "                out[k][kk] = out[k].get(kk, 0) + vv\n",
    "    return out\n",
    "\n",
    "\n",
    "for task, benchmarks in tqdm(tasks.items()):\n",
    "    # Dictionary for storing results to aggregate by task\n",
    "    task_d = {}\n",
    "\n",
    "    print(f\"Aggregating for {task} logs\")\n",
    "    for benchmark in benchmarks:\n",
    "\n",
    "        print(f\"Working on {benchmark} now...\")\n",
    "        final_df, aggregated_dict = obtain_benchmark_task_csv(benchmark, task)\n",
    "\n",
    "        benchmark_dict = {}\n",
    "\n",
    "        for model, mut_cat_dict in aggregated_dict.items():\n",
    "            if \"ensemble\" in model:\n",
    "                continue\n",
    "\n",
    "            for mut_cat, res_dir in mut_cat_dict.items():\n",
    "                # make a NEW dict here instead of aliasing res_dir\n",
    "                if not benchmark_dict.get(mut_cat, None):\n",
    "                    benchmark_dict[mut_cat] = res_dir.copy()\n",
    "                else:\n",
    "                    for key, val in res_dir.items():\n",
    "                        benchmark_dict[mut_cat][key] += val\n",
    "\n",
    "            d1 = task_d.get(task, {})\n",
    "            if not d1:\n",
    "                task_d[task] = copy.deepcopy(mut_cat_dict)\n",
    "            else:\n",
    "                task_d[task] = combine_two_dictionaries(d1, mut_cat_dict)\n",
    "                \n",
    "        # building benchmark dict for aggregating results by benchmark\n",
    "        d = all_benchmark_dict.get(benchmark, {})\n",
    "        if not d:\n",
    "            all_benchmark_dict[benchmark] = benchmark_dict\n",
    "        else:\n",
    "            new_d =  combine_two_dictionaries(d, benchmark_dict) \n",
    "            all_benchmark_dict[benchmark] = new_d\n",
    "\n",
    "        # building overall dictionary for aggregating results by models\n",
    "        if not overall_dict:\n",
    "            overall_dict = aggregated_dict\n",
    "        else:\n",
    "            for model_name, dict1 in overall_dict.items():\n",
    "                dict2 = aggregated_dict[model_name]\n",
    "                overall_dict[model_name] = combine_two_dictionaries(dict1, dict2)\n",
    "    \n",
    "    \n",
    "    task_dict[task] = task_d[task]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Aggregated MuCoCo Results Aggregated Across Benchmarks (Table VIII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "benchmark_df = pd.DataFrame()\n",
    "all_cat_dict = {}\n",
    "\n",
    "\n",
    "for benchmark, mut_cat_dict in all_benchmark_dict.items():\n",
    "    benchmark_inconsistencies = 0\n",
    "    benchmark_questions = 0\n",
    "    benchmark_success = 0\n",
    "    benchmark_answered = 0\n",
    "\n",
    "    for mut_cat, res_dir in mut_cat_dict.items():\n",
    "        if not res_dir:\n",
    "            continue\n",
    "\n",
    "        # Make a defensive copy so we don’t mutate shared references\n",
    "        res_dir = copy.deepcopy(res_dir)\n",
    "\n",
    "        mut_inconsistencies = res_dir['total_inconsistencies']\n",
    "        mut_questions = res_dir['total_questions']\n",
    "        mut_success = res_dir['total_success']\n",
    "        mut_answered = res_dir['total_answered']\n",
    "\n",
    "        benchmark_df.loc[mut_cat, f\"{benchmark} Inconsistencies\"] = f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "        benchmark_df.loc[mut_cat, f\"{benchmark} Accuracy\"] = f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)}%)\"\n",
    "\n",
    "        benchmark_inconsistencies += mut_inconsistencies\n",
    "        benchmark_questions += mut_questions\n",
    "        benchmark_success += mut_success\n",
    "        benchmark_answered += mut_answered\n",
    "\n",
    "        d = all_cat_dict.get(mut_cat, {})\n",
    "        if not d:\n",
    "            all_cat_dict[mut_cat] = copy.deepcopy(res_dir)\n",
    "        else:\n",
    "            for key, value in d.items():\n",
    "                d[key] += res_dir[key]\n",
    "            all_cat_dict[mut_cat] = d\n",
    "\n",
    "\n",
    "    benchmark_df.loc[\"Aggregated Results\", f\"{benchmark} Inconsistencies\"] = f\"{benchmark_inconsistencies}/{benchmark_questions} ({round(benchmark_inconsistencies*100/benchmark_questions, 2)})\"\n",
    "    benchmark_df.loc[\"Aggregated Results\", f\"{benchmark} Accuracy\"] = f\"{benchmark_success}/{benchmark_answered} ({round(benchmark_success*100/benchmark_answered, 2)}%)\"\n",
    "\n",
    "for mut_cat, res_dict in all_cat_dict.items():\n",
    "    mut_inconsistencies = res_dict['total_inconsistencies']\n",
    "    mut_questions = res_dict['total_questions']\n",
    "    mut_success = res_dict['total_success']\n",
    "    mut_answered = res_dict['total_answered']\n",
    "    benchmark_df.loc[mut_cat, \"Aggregated Mutation Inc.\"] =  f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "    benchmark_df.loc[mut_cat, \"Aggregated Mutation Acc.\"] =  f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)})\"\n",
    "    \n",
    "\n",
    "print(benchmark_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# MuCoCo Results Aggregated Across Tasks (Table VII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "task_df = pd.DataFrame()\n",
    "all_cat_dict = {}\n",
    "\n",
    "\n",
    "for task, mut_cat_dict in task_dict.items():\n",
    "\n",
    "    benchmark_inconsistencies = 0\n",
    "    benchmark_questions = 0\n",
    "    benchmark_success = 0\n",
    "    benchmark_answered = 0\n",
    "\n",
    "    for mut_cat, res_dir in mut_cat_dict.items():\n",
    "        if not res_dir:\n",
    "            continue\n",
    "\n",
    "        # Make a defensive copy so we don’t mutate shared references\n",
    "        res_dir = copy.deepcopy(res_dir)\n",
    "\n",
    "        mut_inconsistencies = res_dir['total_inconsistencies']\n",
    "        mut_questions = res_dir['total_questions']\n",
    "        mut_success = res_dir['total_success']\n",
    "        mut_answered = res_dir['total_answered']\n",
    "\n",
    "        task_df.loc[mut_cat, f\"{task} Inconsistencies\"] = f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "        task_df.loc[mut_cat, f\"{task} Accuracy\"] = f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)}%)\"\n",
    "\n",
    "        benchmark_inconsistencies += mut_inconsistencies\n",
    "        benchmark_questions += mut_questions\n",
    "        benchmark_success += mut_success\n",
    "        benchmark_answered += mut_answered\n",
    "\n",
    "        d = all_cat_dict.get(mut_cat, {})\n",
    "        if not d:\n",
    "            all_cat_dict[mut_cat] = copy.deepcopy(res_dir)\n",
    "        else:\n",
    "            for key, value in d.items():\n",
    "                d[key] += res_dir[key]\n",
    "            all_cat_dict[mut_cat] = d\n",
    "\n",
    "\n",
    "    task_df.loc[\"Aggregated Results\", f\"{task} Inconsistencies\"] = f\"{benchmark_inconsistencies}/{benchmark_questions} ({round(benchmark_inconsistencies*100/benchmark_questions, 2)})\"\n",
    "    task_df.loc[\"Aggregated Results\", f\"{task} Accuracy\"] = f\"{benchmark_success}/{benchmark_answered} ({round(benchmark_success*100/benchmark_answered, 2)}%)\"\n",
    "\n",
    "\n",
    "for mut_cat, res_dict in all_cat_dict.items():\n",
    "    mut_inconsistencies = res_dict['total_inconsistencies']\n",
    "    mut_questions = res_dict['total_questions']\n",
    "    mut_success = res_dict['total_success']\n",
    "    mut_answered = res_dict['total_answered']\n",
    "    task_df.loc[mut_cat, \"Aggregated Mutation Inc.\"] =  f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "    task_df.loc[mut_cat, \"Aggregated Mutation Acc.\"] =  f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)})\"\n",
    "    \n",
    "\n",
    "print(task_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Aggregating MuCoCo results across models (Table VI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = overall_dict\n",
    "categories = [\"Logical\", \"Syntactic\", \"Lexical\"]\n",
    "models = list(data.keys())\n",
    "\n",
    "rows = []\n",
    "\n",
    "# keep track of total inconsistencies per model\n",
    "model_inconsistency_totals = {model: 0 for model in models}\n",
    "\n",
    "for cat in categories:\n",
    "    row = {\"Category\": cat}\n",
    "    cat_inconsistency = 0\n",
    "    cat_total_questions = 0\n",
    "    cat_total_success = 0\n",
    "    cat_total_answered = 0\n",
    "\n",
    "    for model in models:\n",
    "        vals = data[model][cat]\n",
    "\n",
    "        # string representation for reporting\n",
    "        inc = f'{vals[\"total_inconsistencies\"]}/{vals[\"total_questions\"]} = {round(vals[\"total_inconsistencies\"] / vals[\"total_questions\"] * 100, 2)}'\n",
    "        acc = f'{vals[\"total_success\"]}/{vals[\"total_answered\"]} = {round(vals[\"total_success\"] / vals[\"total_answered\"] * 100, 2)}'\n",
    "\n",
    "        row[f\"{model} Inconsistency\"] = inc\n",
    "        row[f\"{model} Accuracy\"] = acc\n",
    "\n",
    "        # accumulate for averages\n",
    "        if \"ensemble\" not in model:\n",
    "            cat_inconsistency += vals[\"total_inconsistencies\"]\n",
    "            cat_total_questions += vals[\"total_questions\"]\n",
    "            cat_total_success += vals[\"total_success\"]\n",
    "            cat_total_answered += vals[\"total_answered\"]\n",
    "\n",
    "        # accumulate for global weightage\n",
    "        model_inconsistency_totals[model] += vals[\"total_inconsistencies\"]\n",
    "\n",
    "    # per-category average\n",
    "    row[\"Average Inconsistency\"] = f\"{cat_inconsistency}/{cat_total_questions} = {round(cat_inconsistency*100 / cat_total_questions, 2)}\"\n",
    "    row[\"Average Accuracy\"] = f\"{cat_total_success}/{cat_total_answered} = {round(cat_total_success*100 / cat_total_answered, 2)}\"\n",
    "    rows.append(row)\n",
    "\n",
    "avg_row = {\"Category\": \"All\"}\n",
    "sums = {col: {\"num\": 0, \"den\": 0} for col in rows[0].keys() if col != \"Category\"}\n",
    "\n",
    "for row in rows:\n",
    "    for col in sums.keys():\n",
    "        val = row[col]\n",
    "        if isinstance(val, str) and \"/\" in val:\n",
    "            try:\n",
    "                frac_part = val.split('=')[0].strip()\n",
    "                num, den = frac_part.split('/')\n",
    "                num, den = int(num.strip()), int(den.strip())\n",
    "                sums[col][\"num\"] += num\n",
    "                sums[col][\"den\"] += den\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "for col, vals in sums.items():\n",
    "    num, den = vals[\"num\"], vals[\"den\"]\n",
    "    if den > 0:\n",
    "        avg_row[col] = f\"{num}/{den} = {round(num * 100 / den, 2)}\"\n",
    "    else:\n",
    "        avg_row[col] = \"0/0 = 0.0\"\n",
    "\n",
    "rows.append(avg_row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Formulation of Model Weights used for weighted model ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "valid_models = [m for m in models if \"ensemble\" not in m]\n",
    "\n",
    "raw = np.array([model_inconsistency_totals[m] for m in valid_models], dtype=float)\n",
    "\n",
    "inv = 1 / raw\n",
    "\n",
    "weights = inv / np.sum(inv)\n",
    "\n",
    "df_weights = pd.DataFrame({\n",
    "    \"Model\": valid_models,\n",
    "    \"Inverse Weight\": np.round(weights, 6)\n",
    "}).sort_values(by=\"Inverse Weight\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Sum of weights:\", np.sum(df_weights[\"Inverse Weight\"]))\n",
    "print(df_weights.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
