{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MuCoCo RQ2 Experiment Results Aggregation\n",
    "\n",
    "This notebook is used to aggregate the results for MuCoCo RQ2 experiments. The results are stored in MuCoCo_results/MuCoCo_experiment_results/ in the project root folder. The final aggregated results from this notebook are used in tables X (Effectiveness of MuCoCo VS Turbulence using the Turbulence Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline.turbulence_benchmark.utility.turbulence_log_functions import TurbulenceLogHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_aggregated_results(d: List[Dict]) -> Dict[str, int]:\n",
    "    results = {\n",
    "        \"correct\": 0,\n",
    "        \"num_tasks\": 0,\n",
    "        \"num_questions\": 0,\n",
    "        \"inconsistencies\": 0,\n",
    "        \"total_comparisons\": 0,\n",
    "        \"question_inconsistencies\": 0\n",
    "    }\n",
    "\n",
    "    for res in d:\n",
    "        results[\"correct\"] += res.get(\"correct_instances\", 0)\n",
    "        results[\"num_tasks\"] += res.get(\"correct_instances\", 0) + res.get(\"incorrect_instances\", 0)\n",
    "        results[\"num_questions\"] += res.get(\"total_questions\", 0)\n",
    "        results[\"inconsistencies\"] += res.get(\"inconsistency_count\", 0)\n",
    "        results[\"total_comparisons\"] += res.get(\"total_comparisons\", 0)\n",
    "        results['question_inconsistencies'] += res.get(\"inconsistent_qn_count\", 0)\n",
    "\n",
    "    print(results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_table(res_dir: str):\n",
    "\n",
    "    \"\"\"\n",
    "    This method assumes that all logs are filled. \n",
    "    \"\"\"\n",
    "\n",
    "    csv_logs = [f for f in os.listdir(res_dir) if \n",
    "        os.path.isfile(os.path.join(res_dir, f)) and \n",
    "        f.endswith(\".csv\") and \n",
    "        \"turbulence\" in f.lower()\n",
    "        ]\n",
    "        \n",
    "    no_mutation_log_name = [f for f in csv_logs if \"no_mutation\" in f][-1]\n",
    "    random_log_name = [f for f in csv_logs if \"random\" in f][-1]\n",
    "    sequential_log_name = [f for f in csv_logs if \"sequential\" in f][-1]      \n",
    "\n",
    "    if \"code_generation\" not in res_dir:\n",
    "\n",
    "        for2while_log_name = [f for f in csv_logs if \"for2while\" in f][-1]      \n",
    "        for2enumerate_log_name = [f for f in csv_logs if \"for2enumerate\" in f][-1]\n",
    "        literal_format_log_name = [f for f in csv_logs if \"literal_format\" in f][-1]\n",
    "        boolean_literal_log_name = [f for f in csv_logs if \"boolean_literal\" in f][-1]\n",
    "        commutative_reorder_log_name = [f for f in csv_logs if \"commutative_reorder\" in f][-1]\n",
    "        demorgan_log_name = [f for f in csv_logs if \"demorgan\" in f][-1]\n",
    "        const_unfold_log_name = [f for f in csv_logs if \"constant_unfold\" in f and not any(suffix in f for suffix in [\"constant_unfold_add\", \"constant_unfold_mult\"])][-1]\n",
    "        const_unfold_multi_log_name = [f for f in csv_logs if \"constant_unfold_add\" in f][-1]\n",
    "        const_unfold_add_log_name = [f for f in csv_logs if \"constant_unfold_mult\" in f][-1]\n",
    "        \n",
    "        log_names = [no_mutation_log_name, random_log_name, sequential_log_name, for2while_log_name, for2enumerate_log_name, literal_format_log_name, boolean_literal_log_name, commutative_reorder_log_name, demorgan_log_name, const_unfold_log_name, const_unfold_add_log_name, const_unfold_multi_log_name]\n",
    "        \n",
    "    else:\n",
    "        log_names = [no_mutation_log_name, random_log_name, sequential_log_name]\n",
    "    \n",
    "\n",
    "    helper = TurbulenceLogHelper()\n",
    "\n",
    "    res_df = pd.DataFrame()\n",
    "\n",
    "    total_dict = []\n",
    "\n",
    "\n",
    "    for log_name in log_names:\n",
    "        # print(log_name)\n",
    "        \n",
    "        log = pd.read_csv(os.path.join(res_dir, log_name))\n",
    "\n",
    "        log_dict: Dict = helper.obtain_turbulence_code_inconsistency_score(log)\n",
    "        log_qn_inconsistency_dict = helper.obtain_question_inconsistency_count(log = log)\n",
    "        \n",
    "        log_dict.update(log_qn_inconsistency_dict)\n",
    "\n",
    "        if 'no_mutation' not in log_name:\n",
    "            total_dict.append(log_dict)\n",
    "        else:\n",
    "            res_df.loc[log_name.capitalize().split(\".csv\")[0],\"#errs\"] = f\"{log_dict['inconsistency_count']}\"\n",
    "            res_df.loc[log_name.capitalize().split(\".csv\")[0],\"#tests\"] = f\"{log_dict['total_comparisons']}\"\n",
    "            res_df.loc[log_name.capitalize().split(\".csv\")[0],\"Code Inconsistency Score\"] = f\"{log_dict['inconsistency_count']}/{log_dict['total_comparisons']} ({round(log_dict['inconsistency_count']*100 / log_dict['total_comparisons'], 2)}%)\"\n",
    "\n",
    "\n",
    "    # print(lexical_dicts, logical_dicts)\n",
    "\n",
    "    title = \"MuCoCo\"\n",
    "\n",
    "    dict_df = obtain_aggregated_results(total_dict)\n",
    "    inconsistencies = dict_df.get(\"inconsistencies\", 0)\n",
    "    total_comparisons = dict_df.get(\"total_comparisons\", 0)\n",
    "    \n",
    "    inconsistency_pct = round(inconsistencies * 100 / total_comparisons, 2)\n",
    "\n",
    "    # Write to DataFrame\n",
    "    res_df.loc[title,\"#errs\"] = f\"{inconsistencies}\"\n",
    "    res_df.loc[title,\"#tests\"] = f\"{total_comparisons}\"\n",
    "    res_df.loc[title, \"Code Inconsistency Score\"] = f\"{inconsistencies}/{total_comparisons} ({inconsistency_pct}%)\"\n",
    "        \n",
    "    return res_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Code Generation Turbulence VS MuCoCo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "proj_dir = os.path.abspath(os.path.join(current_dir, \"..\",))\n",
    "res_dir = os.path.join(proj_dir, \"MuCoCO_experiment_results/code_generation/gpt-4o\")\n",
    "\n",
    "res_df = generate_results_table(res_dir=res_dir)\n",
    "print(res_df.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Input Prediction Turbulence VS MuCoCo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = os.path.join(proj_dir, \"MuCoCO_experiment_results/input_prediction/gpt-4o\")\n",
    "\n",
    "res_df = generate_results_table(res_dir=res_dir)\n",
    "print(res_df.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Output Prediction Turbulence VS MuCoCo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = os.path.join(proj_dir, \"MuCoCO_experiment_results/output_prediction/gpt-4o\")\n",
    "\n",
    "res_df = generate_results_table(res_dir=res_dir)\n",
    "print(res_df.to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
