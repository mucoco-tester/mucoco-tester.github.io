{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MuCoCo RQ3 Experiment Results Aggregation for Mutation Operators\n",
    "\n",
    "This notebook is used to aggregate the results for MuCoCo results shown in Figure 2. The results are stored in MuCoCo_results/MuCoCo_experiment_results/ in the project root folder. The final aggregated results from this notebook are used in Figure 2 in the report (consistency error rate and accuracy by mutation operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.data_log_functions import DataLogHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_two_df(df1: pd.DataFrame, df2: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    common_ids = set(df1[\"task_id\"]) & set(df2[\"task_id\"])\n",
    "    if not common_ids:\n",
    "        print(\"⚠️ No matching task_ids found between the two DataFrames.\")\n",
    "        return df1.iloc[0:0], df2.iloc[0:0]  # return empty aligned frames\n",
    "\n",
    "    df1_filtered = df1[df1[\"task_id\"].isin(common_ids)].copy()\n",
    "    df2_filtered = df2[df2[\"task_id\"].isin(common_ids)].copy()\n",
    "\n",
    "    df1_filtered = df1_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "    df2_filtered = df2_filtered.drop_duplicates(subset=[\"task_id\"], keep=\"first\")\n",
    "\n",
    "    df1_filtered = df1_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "    df2_filtered = df2_filtered.sort_values(\"task_id\").reset_index(drop=True)\n",
    "\n",
    "    return df1_filtered, df2_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_code_generation_logs(res_dir: str, filter: Tuple[str] = (), anti_filter: Tuple[str] = ()):\n",
    "    \n",
    "    if filter is None:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (os.path.isfile(os.path.join(res_dir, f)) and f.endswith(\".csv\"))]\n",
    "    else:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (\n",
    "            os.path.isfile(os.path.join(res_dir, f)) and \n",
    "            f.endswith(\".csv\") and \n",
    "            all(sub in f for sub in filter)) and\n",
    "            all(sub not in f for sub in anti_filter)\n",
    "            ]\n",
    "\n",
    "    log_file_names = [csv_file_name.replace('.csv', '') for csv_file_name in csv_logs]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=log_file_names, index = log_file_names)\n",
    "    for file_name in log_file_names:\n",
    "        results_df.loc[file_name, file_name] = float('nan')\n",
    "\n",
    "    while len(csv_logs) > 0:\n",
    "        log1_file_name = csv_logs.pop()\n",
    "        for log2_file_name in csv_logs:\n",
    "            log1_file_path = os.path.join(res_dir, log1_file_name)\n",
    "            log2_file_path = os.path.join(res_dir, log2_file_name)\n",
    "\n",
    "            log1 = pd.read_csv(log1_file_path)\n",
    "            log2 = pd.read_csv(log2_file_path) \n",
    "            print(log1_file_name, log2_file_name)\n",
    "            log1, log2 = standardize_two_df(log1, log2)\n",
    "            log1_inconsistencies, log2_inconsistencies = DataLogHelper.compare_code_generation_dataframe_results(log1=log1, log2=log2)\n",
    "\n",
    "            results_df.loc[log1_file_name.replace('.csv', ''), log2_file_name.replace('.csv', '')] = log1_inconsistencies\n",
    "            results_df.loc[log2_file_name.replace('.csv', ''), log1_file_name.replace('.csv', '')] = log2_inconsistencies\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_csv_name(file_name: str)-> str:\n",
    "    mutation_type = file_name.split(\"shot_\")[-1]\n",
    "    if \"_\" in mutation_type:\n",
    "        mutation = mutation_type.replace(\"_\", \" \").title()\n",
    "        return mutation\n",
    "    return mutation_type.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_category(log_name:str) -> str | None:\n",
    "    mutation_categories = {\n",
    "        \"Lexical\": [\n",
    "            \"literal_format\",\n",
    "            \"random\",\n",
    "            \"sequential\"\n",
    "        ],\n",
    "        \"Syntactic\": [\n",
    "            \"for2while\",\n",
    "            \"for2enumerate\"\n",
    "        ],\n",
    "        \"Logical\": [\n",
    "            \"boolean_literal\",\n",
    "            \"constant_unfold\",\n",
    "            \"constant_unfold_add\",\n",
    "            \"constant_unfold_mult\",\n",
    "            \"demorgan\",\n",
    "            \"commutative_reorder\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for cat, mut in mutation_categories.items():\n",
    "        for m in mut:\n",
    "            m_split = m.split(\"_\")\n",
    "            if m_split[-1] in log_name.split(\"_\")[-1]:\n",
    "                return m\n",
    "            elif 'no_mutation' in log_name:\n",
    "                return 'no_mutation'    \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_logs_against_no_mutation(res_dir: str, filter: Tuple[str] = (), anti_filter: Tuple[str] = ()):\n",
    "    \n",
    "    if filter is None:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (os.path.isfile(os.path.join(res_dir, f)) and f.endswith(\".csv\"))]\n",
    "    else:\n",
    "        csv_logs = [f for f in os.listdir(res_dir) if (\n",
    "            os.path.isfile(os.path.join(res_dir, f)) and \n",
    "            f.endswith(\".csv\") and \n",
    "            all(sub in f for sub in filter)) and\n",
    "            all(sub not in f for sub in anti_filter)\n",
    "            ]\n",
    "\n",
    "    csv_logs.sort()\n",
    "    target_log_name = [l for l in csv_logs if \"no_mutation\" in l][-1]\n",
    "    csv_logs.pop(csv_logs.index(target_log_name))\n",
    "    target_log_path = os.path.join(res_dir, target_log_name)\n",
    "    target_log = pd.read_csv(target_log_path)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    total_inconsistencies = 0\n",
    "    total_questions = 0\n",
    "    total_success = 0\n",
    "    total_answered = 0\n",
    "\n",
    "    category_dict = {}\n",
    "    mutation_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "    for log_name in csv_logs:\n",
    "        # print(log_name)\n",
    "\n",
    "        log_category = obtain_category(log_name)\n",
    "                \n",
    "        log2_file_path = os.path.join(res_dir, log_name)\n",
    "        log2 = pd.read_csv(log2_file_path) \n",
    "\n",
    "        target_log, log2 = standardize_two_df(target_log, log2)\n",
    "\n",
    "        inconsistency_dict = DataLogHelper.compare_code_generation_dataframe_results(log1=target_log, log2=log2)\n",
    "\n",
    "        # Adding results into the dataframe\n",
    "        cleaned_mutation_name = clean_up_csv_name(log_name.replace('.csv', ''))\n",
    "        results_df.loc[cleaned_mutation_name, \"Inconsistency Score\"] = f\"{inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']}/{inconsistency_dict['total_inconsistency_questions']} ({round((inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies'])*100/inconsistency_dict['total_inconsistency_questions'], 2)}%)\"\n",
    "        results_df.loc['No Mutation', \"Inconsistency Score\"] = \"N/A\"\n",
    "        results_df.loc['No Mutation', \"Model Accuracy\"] = f\"{(inconsistency_dict['log1_success'])}/{inconsistency_dict['log1_total_answered']} ({round((inconsistency_dict['log1_success'])*100/inconsistency_dict['log1_total_answered'], 2)}%)\"\n",
    "        results_df.loc[cleaned_mutation_name, \"Model Accuracy\"] = f\"{(inconsistency_dict['log2_success'])}/{inconsistency_dict['log2_total_answered']} ({round((inconsistency_dict['log2_success'])*100/inconsistency_dict['log2_total_answered'], 2)}%)\"\n",
    "\n",
    "        if total_success == 0:\n",
    "            total_success += inconsistency_dict['log1_success']\n",
    "        \n",
    "        if total_answered == 0:\n",
    "            total_answered += inconsistency_dict['log1_total_answered']\n",
    "\n",
    "        if 'model_ensemble' in log_name.lower() or \"ensemble\" not in log_name.lower() :\n",
    "            total_inconsistencies += inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']\n",
    "            total_questions += inconsistency_dict['total_inconsistency_questions']\n",
    "            total_success += inconsistency_dict['log2_success']\n",
    "            total_answered += inconsistency_dict['log2_total_answered']\n",
    "\n",
    "        \n",
    "        if log_category:\n",
    "            d: Dict = category_dict.get(log_category, {})\n",
    "            d['total_inconsistencies'] = d.get('total_inconsistencies', 0) + inconsistency_dict['log1_inconsistencies'] + inconsistency_dict['log2_inconsistencies']\n",
    "            d['total_questions'] = d.get('total_questions', 0) + inconsistency_dict['total_inconsistency_questions']\n",
    "            d['total_success'] = d.get('total_success', 0) + inconsistency_dict['log2_success']\n",
    "            d['total_answered'] = d.get('total_answered', 0) + inconsistency_dict['log2_total_answered']\n",
    "            category_dict[log_category] = d\n",
    "\n",
    "        if not category_dict.get('no_mutation', None):\n",
    "            d2 = {}\n",
    "            d2['total_inconsistencies'] = 0\n",
    "            d2['total_questions'] = 1\n",
    "            d2['total_success'] = inconsistency_dict['log1_success']\n",
    "            d2['total_answered'] = inconsistency_dict['log1_total_answered']\n",
    "            category_dict['no_mutation'] = d2\n",
    "\n",
    "\n",
    "        # adding results in mutation_dict, with the mutation name as key\n",
    "        mutation_dict[cleaned_mutation_name] = {\n",
    "            'total_inconsistencies': inconsistency_dict['log1_inconsistencies']+ inconsistency_dict['log2_inconsistencies'],\n",
    "            'total_questions': inconsistency_dict['total_inconsistency_questions'],\n",
    "            'total_success': inconsistency_dict['log2_success'],\n",
    "            'total_answered': inconsistency_dict['log2_total_answered']\n",
    "        }\n",
    "    \n",
    "    results_df = pd.concat([\n",
    "        results_df[results_df.index.str.lower().str.contains(\"no mutation\")],\n",
    "\n",
    "        results_df[\n",
    "            ~results_df.index.str.lower().str.contains(\"ensemble\") &\n",
    "            ~results_df.index.str.lower().str.contains(\"no mutation\")\n",
    "        ],\n",
    "\n",
    "        results_df[results_df.index.str.lower().str.contains(\"ensemble\")]\n",
    "    ])\n",
    "\n",
    "    ## Adding aggregated second order results and atomic results\n",
    "    for key, mut_dict in category_dict.items():\n",
    "        mut_inconsistencies = mut_dict['total_inconsistencies']\n",
    "        mut_questions = mut_dict['total_questions']\n",
    "        mut_success = mut_dict['total_success']\n",
    "        mut_answered = mut_dict['total_answered']\n",
    "        results_df.loc[f\"{key} Results\", \"Inconsistency Score\"] = f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "        results_df.loc[f\"{key} Results\", \"Model Accuracy\"] = f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)}%)\"\n",
    "\n",
    "        mutation_dict[f\"{key} Results\"] = mut_dict\n",
    "\n",
    "\n",
    "    results_df.loc[\"Aggregated Results\", \"Inconsistency Score\"] = f\"{total_inconsistencies}/{total_questions} ({round(total_inconsistencies*100/total_questions, 2)})\"\n",
    "    results_df.loc[\"Aggregated Results\", \"Model Accuracy\"] = f\"{total_success}/{total_answered} ({round(total_success*100/total_answered, 2)}%)\"\n",
    "\n",
    "\n",
    "    return [\n",
    "        results_df, \n",
    "        category_dict, \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"Qwen2.5-Coder-14B-Instruct\" : \"Qwen2.5-Coder-14B-Instruct\",\n",
    "    \"gemma-3-12b-it\": \"Gemma-3-12b-it\",\n",
    "    \"deepseek-reasoner\": \"DeepSeek-V3.2-Exp (Non-thinking Mode)\",\n",
    "    \"LLama-3.1-8B\": \"LLama-3.1-8B\",\n",
    "    \"gpt-5\" : \"GPT-5\",\n",
    "    \"gpt-4o\": \"GPT-4o\",\n",
    "    \"codestral-latest\": \"codestral-2508\",\n",
    "}\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "proj_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "def obtain_benchmark_task_csv(benchmark: str, task: str) -> pd.DataFrame:\n",
    "\n",
    "    final_df = pd.DataFrame()  # start with an empty DataFrame\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterating through each model in model_dict\n",
    "    for k, m in model_dict.items():\n",
    "        # print(k)\n",
    "        res_dir = os.path.join(proj_dir, f\"MuCoCo_experiment_results/{task}/{k}\")\n",
    "        try:\n",
    "            res, category_dict = compare_logs_against_no_mutation(res_dir=res_dir, filter=(benchmark, ))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{res_dir} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        res_df = pd.DataFrame(res)\n",
    "\n",
    "        res_df = res_df.add_prefix(f\"{m} \")\n",
    "\n",
    "        if final_df.empty:\n",
    "            final_df = res_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df, res_df], axis=1)\n",
    "\n",
    "        final_dict[m] = category_dict\n",
    "\n",
    "    # final_df.to_csv(\"combined_results.csv\", index=True, header=True)\n",
    "    return final_df, final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "tasks = {\n",
    "    'mcq_inconsistency': ['CodeMMLU'],\n",
    "    'input_prediction': ['HumanEval', \"CruxEval\"],\n",
    "    'output_prediction': ['HumanEval', \"CruxEval\"],\n",
    "    'code_generation': ['BigCodeBench', \"HumanEval\"],\n",
    "}\n",
    "\n",
    "task_dict = {}\n",
    "overall_dict = {}\n",
    "all_benchmark_dict = {}\n",
    "dfs = []\n",
    "\n",
    "def combine_two_dictionaries(d1: dict, d2: dict) -> dict:\n",
    "    # make it PURE (return a new merged dict)\n",
    "    out = copy.deepcopy(d1)\n",
    "    for k, inner2 in d2.items():\n",
    "        if k not in out:\n",
    "            out[k] = copy.deepcopy(inner2)              # shallow is enough at this level\n",
    "        else:\n",
    "            for kk, vv in inner2.items():\n",
    "                out[k][kk] = out[k].get(kk, 0) + vv\n",
    "    return out\n",
    "\n",
    "\n",
    "for task, benchmarks in tqdm(tasks.items()):\n",
    "    # Dictionary for storing results to aggregate by task\n",
    "    task_d = {}\n",
    "\n",
    "    print(f\"Aggregating for {task} logs\")\n",
    "    for benchmark in benchmarks:\n",
    "\n",
    "        print(f\"Working on {benchmark} now...\")\n",
    "        final_df, aggregated_dict = obtain_benchmark_task_csv(benchmark, task)\n",
    "\n",
    "        benchmark_dict = {}\n",
    "\n",
    "        for model, mut_cat_dict in aggregated_dict.items():\n",
    "            if \"ensemble\" in model:\n",
    "                continue\n",
    "            for mut_cat, res_dir in mut_cat_dict.items():\n",
    "                # make a NEW dict here instead of aliasing res_dir\n",
    "                if not benchmark_dict.get(mut_cat, None):\n",
    "                    benchmark_dict[mut_cat] = res_dir.copy()\n",
    "                else:\n",
    "                    for key, val in res_dir.items():\n",
    "                        benchmark_dict[mut_cat][key] += val\n",
    "                    \n",
    "        # building benchmark dict for aggregating results by benchmark\n",
    "        d = all_benchmark_dict.get(benchmark, {})\n",
    "        if not d:\n",
    "            all_benchmark_dict[benchmark] = benchmark_dict\n",
    "        else:\n",
    "            new_d =  combine_two_dictionaries(d, benchmark_dict) \n",
    "            all_benchmark_dict[benchmark] = new_d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Consistency Error Rate and Accuracy by Mutation Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "benchmark_df = pd.DataFrame()\n",
    "all_cat_dict = {}\n",
    "\n",
    "\n",
    "for benchmark, mut_cat_dict in all_benchmark_dict.items():\n",
    "    for mut_cat, res_dir in mut_cat_dict.items():\n",
    "        d = all_cat_dict.get(mut_cat, {})\n",
    "        if not d:\n",
    "            all_cat_dict[mut_cat] = copy.deepcopy(res_dir)\n",
    "        else:\n",
    "            for key, value in d.items():\n",
    "                d[key] += res_dir[key]\n",
    "            all_cat_dict[mut_cat] = d\n",
    "\n",
    "aggregated_mutation_inc = []\n",
    "aggregated_mutation_acc = []\n",
    "\n",
    "mutation_name_map = {\n",
    "    \"no_mutation\": \"no mutation\",\n",
    "    \"boolean_literal\": \"boolean literal\",\n",
    "    \"commutative_reorder\": \"commutative reorder\",\n",
    "    \"constant_unfold\": \"constant unfold\",\n",
    "    \"constant_unfold_add\": \"constant unfold add\",\n",
    "    \"constant_unfold_mult\": \"constant unfold mult\",\n",
    "    \"demorgan\": \"demorgan\",\n",
    "    \"for2enumerate\": \"for-to-enumerate\",\n",
    "    \"for2while\": \"for-to-while\",\n",
    "    \"literal_format\": \"literal format\",\n",
    "    \"random\": \"random\",\n",
    "    \"sequential\": \"sequential\"\n",
    "}\n",
    "\n",
    "for mut_cat, clean_name in mutation_name_map.items():\n",
    "    res_dict = all_cat_dict[mut_cat]\n",
    "    mut_inconsistencies = res_dict['total_inconsistencies']\n",
    "    mut_questions = res_dict['total_questions']\n",
    "    mut_success = res_dict['total_success']\n",
    "    mut_answered = res_dict['total_answered']\n",
    "    benchmark_df.loc[mut_cat, \"Aggregated Inc.\"] =  f\"{mut_inconsistencies}/{mut_questions} ({round(mut_inconsistencies*100/mut_questions, 2)})\"\n",
    "    benchmark_df.loc[mut_cat, \"Aggregated Acc.\"] =  f\"{mut_success}/{mut_answered} ({round(mut_success*100/mut_answered, 2)}%)\"\n",
    "    aggregated_mutation_inc.append(round(mut_inconsistencies*100/mut_questions, 2))\n",
    "    aggregated_mutation_acc.append(round(mut_success*100/mut_answered, 2))\n",
    "print(benchmark_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "    \"mutation_type\": list(mutation_name_map.values()),\n",
    "    \"Aggregated Mutation Inc.\": aggregated_mutation_inc,\n",
    "    \"Aggregated Mutation Acc.\": aggregated_mutation_acc,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Setup\n",
    "x = np.arange(len(df[\"mutation_type\"]))  # positions\n",
    "width = 0.38  # width of bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bars\n",
    "bars1 = ax.bar(x - width/2, df[\"Aggregated Mutation Inc.\"], width, label=\"Inconsistency (%)\", color=\"tomato\", alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, df[\"Aggregated Mutation Acc.\"], width, label=\"Accuracy (%)\", color=\"steelblue\", alpha=0.85)\n",
    "\n",
    "# Labels\n",
    "ax.set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "ax.set_xlabel(\"Mutation Type\", fontsize=12)\n",
    "ax.set_title(\"Inconsistency vs Accuracy per Mutation Type\", fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df[\"mutation_type\"], rotation=45, ha=\"right\", fontsize=9)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend()\n",
    "\n",
    "# Optional grid\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
