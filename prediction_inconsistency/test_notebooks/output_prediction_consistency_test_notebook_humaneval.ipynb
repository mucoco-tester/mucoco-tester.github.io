{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## LLM Consistency Testing with Mistral LLM\n",
    "\n",
    "This notebook contains code for testing code inconsistency in Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "par_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(par_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_inconsistency.prediction_inconsistency_tester import LLMConsistencyTester\n",
    "from prediction_inconsistency.prompt_templates.prompt_template import PredictionInconsistencyPromptTemplate\n",
    "from utility.constants import Tasks, PromptTypes, LexicalMutations, SyntacticMutations, LogicalMutations, ReasoningModels, NonReasoningModels, CruxEval, HumanEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Declaring constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Declaring Task Type Constants\n",
    "OUTPUT_PREDICTION = Tasks.OutputPrediction.NAME\n",
    "INPUT_PREDICTION = Tasks.InputPrediction.NAME\n",
    "\n",
    "## Declaring Benchmark Name Constants\n",
    "CRUXEVAL = CruxEval.NAME\n",
    "HUMANEVAL = HumanEval.NAME\n",
    "\n",
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "FEW_SHOT = PromptTypes.FEW_SHOT\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "FOR2WHILE = SyntacticMutations.FOR2WHILE\n",
    "FOR2ENUMERATE = SyntacticMutations.FOR2ENUMERATE\n",
    "\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "BOOLEAN_LITERAL = LogicalMutations.BOOLEAN_LITERAL\n",
    "DEMORGAN = LogicalMutations.DEMORGAN\n",
    "COMMUTATIVE_REORDER = LogicalMutations.COMMUTATIVE_REORDER\n",
    "CONSTANT_UNFOLD = LogicalMutations.CONSTANT_UNFOLD\n",
    "CONSTANT_UNFOLD_ADD = LogicalMutations.CONSTANT_UNFOLD_ADD\n",
    "CONSTANT_UNFOLD_MULT = LogicalMutations.CONSTANT_UNFOLD_MULT\n",
    "\n",
    "## Declaring Reasoning Model Name Constants\n",
    "GPT5 = ReasoningModels.GPT5['name']\n",
    "\n",
    "## Declaring Non-Reasoning Model Name Constants\n",
    "CODESTRAL = NonReasoningModels.CODESTRAL['name']\n",
    "GPT4O = NonReasoningModels.GPT4O['name']\n",
    "DEEPSEEK = NonReasoningModels.DEEPSEEK_CHAT['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = HUMANEVAL\n",
    "database_name = f\"{task_set}_Input_Output\"\n",
    "llmtester = LLMConsistencyTester(database_name, n =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_models = [getattr(ReasoningModels, model) for model in dir(ReasoningModels) if not model.startswith(\"_\")]\n",
    "non_reasoning_models = [getattr(NonReasoningModels, model) for model in dir(NonReasoningModels) if not model.startswith(\"_\")]\n",
    "print('Reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")\n",
    "print('=' * 50)\n",
    "print('Non-reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(non_reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = llmtester.question_database.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The following example runs an output prediction inconsistency test on the HumanEval benchmark for all tasks in HumanEval. To add mutations such as Random mutation, add the corresponding mutation string to the `mutations` list like so: `mutations = [RANDOM_MUTATION]`.  The mutations available for prediction inconsistency testing have been declared as constants above.\n",
    "\n",
    "Kindly refer to `input_prediction_consistency_test_notebook_humaneval.ipynb` under the same parent directory for more details on the function parameters for HumanEval prediction inconsistency testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%script false --no-raise-error\n",
    "mutations = []\n",
    "prompt_type = FEW_SHOT\n",
    "model_name = GPT4O\n",
    "task_type = OUTPUT_PREDICTION \n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "\n",
    "\n",
    "results_dir =os.path.join(proj_dir, f'results', task_type, model_name)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "output_file_path=f\"{results_dir}/{task_set}_{prompt_type}_{mutation_str}.csv\"\n",
    "\n",
    "pass_count = llmtester.run_code_consistency_test(\n",
    "    prompt_helper = PredictionInconsistencyPromptTemplate.return_appropriate_prompt(task_type, prompt_type),\n",
    "    num_tests=num_tests,\n",
    "    prompt_type= prompt_type,\n",
    "    mutations=mutations,\n",
    "    output_file_path=output_file_path,\n",
    "    task_set = task_set,\n",
    "    task_type=task_type,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
