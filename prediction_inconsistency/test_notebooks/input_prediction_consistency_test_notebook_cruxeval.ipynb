{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## LLM Consistency Testing with Mistral LLM\n",
    "\n",
    "This notebook contains code for testing code inconsistency in Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "par_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(par_dir)\n",
    "sys.path.append(proj_dir)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_inconsistency.prediction_inconsistency_tester import LLMConsistencyTester\n",
    "from prediction_inconsistency.prompt_templates.prompt_template import PredictionInconsistencyPromptTemplate, ReasoningPredictionInconsistencyPromptTemplate\n",
    "from utility.constants import Tasks, PromptTypes, LexicalMutations, SyntacticMutations, LogicalMutations, ReasoningModels, NonReasoningModels, CruxEval, HumanEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Declaring constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Declaring Task Type Constants\n",
    "OUTPUT_PREDICTION = Tasks.OutputPrediction.NAME\n",
    "INPUT_PREDICTION = Tasks.InputPrediction.NAME\n",
    "\n",
    "## Declaring Benchmark Name Constants\n",
    "CRUXEVAL = CruxEval.NAME\n",
    "HUMANEVAL = HumanEval.NAME\n",
    "\n",
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "FOR2WHILE = SyntacticMutations.FOR2WHILE\n",
    "FOR2ENUMERATE = SyntacticMutations.FOR2ENUMERATE\n",
    "\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "BOOLEAN_LITERAL = LogicalMutations.BOOLEAN_LITERAL\n",
    "DEMORGAN = LogicalMutations.DEMORGAN\n",
    "COMMUTATIVE_REORDER = LogicalMutations.COMMUTATIVE_REORDER\n",
    "CONSTANT_UNFOLD = LogicalMutations.CONSTANT_UNFOLD\n",
    "CONSTANT_UNFOLD_ADD = LogicalMutations.CONSTANT_UNFOLD_ADD\n",
    "CONSTANT_UNFOLD_MULT = LogicalMutations.CONSTANT_UNFOLD_MULT\n",
    "\n",
    "## Declaring Reasoning Model Name Constants\n",
    "GPT5 = ReasoningModels.GPT5['name']\n",
    "\n",
    "## Declaring Non-Reasoning Model Name Constants\n",
    "CODESTRAL = NonReasoningModels.CODESTRAL['name']\n",
    "GPT4O = NonReasoningModels.GPT4O['name']\n",
    "DEEPSEEK = NonReasoningModels.DEEPSEEK_CHAT['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = CRUXEVAL\n",
    "database_name = os.getenv('MONGODB_CRUXEVAL_COLLECTION')\n",
    "llmtester = LLMConsistencyTester(database_name, n =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_models = [getattr(ReasoningModels, model) for model in dir(ReasoningModels) if not model.startswith(\"_\")]\n",
    "non_reasoning_models = [getattr(NonReasoningModels, model) for model in dir(NonReasoningModels) if not model.startswith(\"_\")]\n",
    "print('Reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")\n",
    "print('=' * 50)\n",
    "print('Non-reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(non_reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = llmtester.question_database.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "`run_code_consistency_test` method is used for running predcition inconsistency tests on MuCoCo.\n",
    "\n",
    "| Parameter              | Type        | Description                                                                                                              |\n",
    "| ---------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `prompt_helper`        | `str`       | String template for the appropriate prompt. Simply rename the `prompt_type` variable to `ZERO_SHOT` for CruxEval.\n",
    "| `output_file_path`     | `str`       | Full path to the CSV where predictions and metrics are saved. Filename is built from model, task type, and mutation tag. |\n",
    "| `num_tests`            | `int`       | Number of test questions to evaluate. Set to `num_tests` to evaluate all tasks in this benchmark.                                                                                 |\n",
    "| `mutations`            | `List[str]` | Mutation operators to apply (e.g., `[\"FOR2WHILE\"]`, `[\"CONSTANT_UNFOLD\"]`). Empty list means **no_mutation**.            |\n",
    "| `model_name`           | `str`       | Identifier of the LLM under test (e.g., `GPT4O`). Used for routing and naming.                                           |\n",
    "| `task_set`      | `str`       | Either  `CRUXEVAL` or `HUMANEVAL` for  prediction inconsistency.                                                           |                    \n",
    "| `take_type`      | `str`       | Either  `OUTPUT_PREDICTION` or `INPUT_PREDICTION` for  prediction inconsistency.                                                           |      \n",
    "| `continue_from_task`   | `str`       | Optional parameter for starting evaluation from a specified task ID corresponding to the task in MongoDB (e.g., `\"CruxEvalTF15\"`).                                                |\n",
    "\n",
    "The following example runs an input prediction inconsistency test on the CruxEval benchmark for all tasks in CruxEval. To add mutations such as Random mutation, add the corresponding mutation string to the `mutations` list like so: `mutations = [RANDOM_MUTATION]`. The mutations available for prediction inconsistency testing have been declared as constants above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "mutations = []\n",
    "prompt_type = ZERO_SHOT\n",
    "model_name = GPT4O\n",
    "task_type = INPUT_PREDICTION \n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "\n",
    "results_dir =os.path.join(proj_dir, f'results', task_type, model_name)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "output_file_path=f\"{results_dir}/{task_set}_{prompt_type}_{mutation_str}_nw1.csv\"\n",
    "\n",
    "pass_count = llmtester.run_code_consistency_test(\n",
    "    prompt_helper = PredictionInconsistencyPromptTemplate.return_appropriate_prompt(task_type, prompt_type),\n",
    "    num_tests= num_tests,\n",
    "    prompt_type= prompt_type,\n",
    "    mutations=mutations,\n",
    "    output_file_path=output_file_path,\n",
    "    task_set = task_set,\n",
    "    task_type=task_type,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
