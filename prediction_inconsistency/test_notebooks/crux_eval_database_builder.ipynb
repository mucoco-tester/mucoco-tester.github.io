{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## MongoDB CruxEval Prediction Inconsistency Database Builder\n",
    "\n",
    "This notebook is used to initialize a new prediction inconsistency database onto MongoDB for CruxEval benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "par_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(par_dir)\n",
    "sys.path.append(proj_dir)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import MongoDBHelper\n",
    "from prediction_inconsistency.utility.cruxeval_helper import PredictionInconsistencyCruxEvalHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MongoDBHelper()\n",
    "if db.check_database_connectivity():\n",
    "    print(\"MongoDB connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_qns_db = db.client[os.getenv('MONGODB_BENCHMARK_DATABASE')]\n",
    "cruxeval_database = pd.read_csv(\n",
    "    os.path.join(proj_dir, \"datasets/open_ended_format/cruxeval_test.csv\"),\n",
    "    encoding=\"utf-8\",\n",
    "    header=0,\n",
    "    quoting=1,           \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_question_database = base_qns_db[os.getenv('MONGODB_CRUXEVAL_COLLECTION')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = set()\n",
    "tot = 0\n",
    "repurposed_qn = {}\n",
    "rej_tot = 0\n",
    "for i in tqdm(range(\n",
    "    len(cruxeval_database)\n",
    "    )):\n",
    "    task_id = f\"CruxEvalTF{i}\"\n",
    "    sample_qn = cruxeval_database.iloc[i]\n",
    "    full_sol = sample_qn['code']\n",
    "    test_input = sample_qn['input']\n",
    "    expected_output = sample_qn['output']\n",
    "    original_id = sample_qn['id']\n",
    "\n",
    "    namespace = {}\n",
    "\n",
    "    func_name  = PredictionInconsistencyCruxEvalHelper.extract_func_name(full_sol)\n",
    "\n",
    "    exec(full_sol, namespace)\n",
    "\n",
    "\n",
    "    try:\n",
    "        test_input = eval(test_input, namespace) if not isinstance(test_input, float) else None\n",
    "        match expected_output:\n",
    "            case \"FALSE\":\n",
    "                expected_output = False\n",
    "            case \"TRUE\":\n",
    "                expected_output = True\n",
    "            case _:\n",
    "                expected_output = eval(expected_output)\n",
    "    except Exception as e:\n",
    "        print(test_input)\n",
    "        print(task_id, f\"failed due to following error: {e}\")\n",
    "        continue\n",
    "\n",
    "    sig = inspect.signature(namespace[func_name])\n",
    "    input_copy = copy.deepcopy(test_input) if isinstance(test_input, (list, dict, tuple, set)) else test_input\n",
    "    try:\n",
    "        if len(sig.parameters) == 0:\n",
    "            assert namespace[func_name]() == expected_output\n",
    "        elif len(sig.parameters) > 1:\n",
    "            assert namespace[func_name](*input_copy) == expected_output\n",
    "        else:\n",
    "            assert namespace[func_name](input_copy) == expected_output\n",
    "\n",
    "    except:\n",
    "        print(f\"Did not pass test case. Double check task_id {task_id}, test_case {test_input}\")\n",
    "    \n",
    "    input_metadata = type(test_input).__name__\n",
    "\n",
    "    func_in_input = None\n",
    "\n",
    "    if input_metadata not in ('str', 'NoneType'):\n",
    "        input_args_tree = ast.parse(sample_qn['input'])\n",
    "        func_in_input = any(isinstance(node, (ast.Call, ast.Lambda)) for node in ast.walk(input_args_tree))\n",
    "\n",
    "    if isinstance(test_input, dict):\n",
    "        test_input = str(test_input)\n",
    "    elif isinstance(test_input, (tuple, list)):\n",
    "        test_input = str(test_input) if any(i for i in test_input if isinstance(i, dict)) else test_input\n",
    "    \n",
    "    ### Storing / updating entry in the database\n",
    "    db_entry = {\n",
    "        \"_id\" : task_id,\n",
    "        \"full_sol\" : full_sol,\n",
    "        \"input\" : {\n",
    "            \"args\": test_input if not func_in_input else sample_qn['input'],\n",
    "            \"metadata\": input_metadata,\n",
    "            },\n",
    "        \"output\": {\n",
    "            \"args\" : str(expected_output),\n",
    "            \"metadata\": type(expected_output).__name__,\n",
    "            },\n",
    "        \"original_id\": original_id\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        tf_question_database.update_one(\n",
    "            filter={\"_id\": task_id},\n",
    "            update={\"$set\": db_entry},\n",
    "            upsert=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not enter test case {original_id} into TF database due to the following error: {e}\")\n",
    "\n",
    "\n",
    "    ## Secondary check where the question is pulled from the database and tested against the check function\n",
    "    ## This step is necessary as MongoDB does not store these details in standard Python data formats and a secondary step is needed for sanity check. \n",
    "    ## For example, tuples are stored as \"arrays\" in MongoDB, which are converted to Lists in Python.\n",
    "    \n",
    "    qn = tf_question_database.find_one({\"_id\" : task_id})\n",
    "    if qn is None:\n",
    "        continue\n",
    "    full_sol = qn['full_sol']                           # full canonical solution for the task\n",
    "\n",
    "    test_inputs = qn['input']                           # unpacking input args and metadata from qn\n",
    "    input_args = test_inputs['args']                    # test input args\n",
    "    input_metadata = test_inputs['metadata']            # test input metadata\n",
    "\n",
    "    test_outputs = qn['output']                         # unpacking outputs args and metadata from qn\n",
    "    output_args = test_outputs['args']                  # test output args\n",
    "    output_metadata = test_outputs['metadata']          # test output metadata\n",
    "\n",
    "    try:\n",
    "        input_args = eval(input_args) if isinstance(input_args, str) and input_metadata != str.__name__  else input_args\n",
    "    except Exception as e:\n",
    "        print(original_id)\n",
    "        print(e)\n",
    "        continue\n",
    "    output_args = ast.literal_eval(output_args) if output_metadata != str.__name__ else output_args\n",
    "\n",
    "    check_stored_soln_validity = PredictionInconsistencyCruxEvalHelper.check_input_output(\n",
    "        full_sol=full_sol,\n",
    "        test_input=input_args,\n",
    "        input_metadata=input_metadata,\n",
    "        expected_output= output_args,\n",
    "        func_name= func_name,\n",
    "    )\n",
    "\n",
    "    if check_stored_soln_validity is not True:\n",
    "        tf_question_database.find_one_and_delete({\"_id\" : task_id})\n",
    "        print(f'{original_id} from {task_id} failed the secondary checks and was not added to the database.')\n",
    "        tot-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "sample_452: modified such that the \",\" behind the initial test input is removed and the single quotation marks replaced with double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{tf_question_database.count_documents({})} total test cases in the database\")\n",
    "print(f'{tot} valid test cases')\n",
    "print(f'{rej_tot} test cases were rejected')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
