{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## MongoDB HumanEval Code Generation Database Builder\n",
    "\n",
    "This notebook is used to initialize a new code generation database onto MongoDB. A custom HumanEval dataset in csv format is processed into a set format through this notebook and stored in your MongoDB cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming running from this notebook directly, else change the file path below.\n",
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "humaneval = pd.read_csv(os.path.join(proj_dir, \"datasets/open_ended_format/humaneval_test_modified_open.csv\"), header = 0, encoding='unicode_escape')\n",
    "\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_generation.utility.humaneval_helper import CodeGenerationHumanEvalHelper\n",
    "from database import MongoDBHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodbHelper = MongoDBHelper()\n",
    "mongodbHelper.check_database_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "db = mongodbHelper.client[os.getenv('MONGODB_BENCHMARK_DATABASE')]\n",
    "open_ended_db = db[os.getenv('MONGODB_HUMANEVAL_CG_COLLECTION')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Running Database Storing Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check_example = []\n",
    "to_check_test = set()\n",
    "\n",
    "for idx in range(humaneval.__len__()):\n",
    "\n",
    "    qn_id = f\"HumanEvalo{idx-len(to_check_example)-len(to_check_test)}\"\n",
    "\n",
    "    qn_details = open_ended_db.find_one({\"_id\" : qn_id})        # checking if this qn_id already exists in the db\n",
    "\n",
    "    qn = humaneval.iloc[idx]\n",
    "    prompt = qn['prompt']\n",
    "    canonical_solution = qn['canonical_solution']\n",
    "    tests = qn['test']\n",
    "    original_test_id = qn['task_id']\n",
    "    func_name = qn['entry_point']\n",
    "\n",
    "    new_qn, qn_desc = CodeGenerationHumanEvalHelper.seperate_original_desciptions(prompt)\n",
    "\n",
    "    desc, examples = CodeGenerationHumanEvalHelper.extract_examples(qn_desc)\n",
    "\n",
    "    ## Rejecting any questions where there are no examples\n",
    "    if len(examples.keys()) < 1:\n",
    "        to_check_example.append(qn_id)\n",
    "        continue\n",
    "\n",
    "    ## Cleaning the check function\n",
    "    check_function = CodeGenerationHumanEvalHelper.process_original_tests(tests)\n",
    "\n",
    "    ## Obtaining the full solution\n",
    "    full_solution = new_qn + \"\\n\" + canonical_solution\n",
    "\n",
    "    ## Ensuring that the full solution passes the check function\n",
    "    code_validation = CodeGenerationHumanEvalHelper.check_test_case(test_case = check_function, code_snippet = full_solution, func_name = func_name)\n",
    "    \n",
    "    entry_dict = {\n",
    "        \"_id\" : qn_id,\n",
    "        \"qn\" : new_qn,\n",
    "        \"canon_solution\" : canonical_solution,\n",
    "        \"qn_desc\" : desc,\n",
    "        \"examples\": examples,\n",
    "        \"check\" : check_function,\n",
    "        \"original_id\": original_test_id,\n",
    "        \"func_name\": func_name\n",
    "    }\n",
    "    if code_validation is True:\n",
    "        if qn_details is None:\n",
    "            open_ended_db.insert_one(entry_dict)\n",
    "            print('Added entry to database: {id}'.format(id = qn_id))\n",
    "        else:\n",
    "            open_ended_db.update_one({\"_id\" : qn_id}, update = {\"$set\": entry_dict})\n",
    "            print('Updated existing entry in database: {id}'.format(id = qn_id))\n",
    "    else:\n",
    "        to_check_test.add(qn_id)\n",
    "\n",
    "print(to_check_example if len(to_check_example) > 0 else \"All cases contains examples. Nothing to check!\")\n",
    "print(to_check_test if len(to_check_test) > 0 else \"All test cases passed. Nothing to check!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- Modified question HumanEval/10 to contain nested functions for uniformity.\n",
    "- HumanEval/41, HumanEval/38, HumanEval/50 did not have any examples in the questions and were excluded during our experimentation. They can be added in for zero shot prompting.\n",
    "- HumanEval/66 to HumanEval/163 modified to fit the standard doctest format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
