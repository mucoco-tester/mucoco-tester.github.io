{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## MuCoCo Code Generation BigCodeBench Benchmark Testing\n",
    "\n",
    "This notebook is used for running experiments for MuCoCo code generation tasks on BigCodeBench benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_generation.code_generation_tester import CodeGenerationTester\n",
    "from code_generation.prompt_templates.prompt_template import OpenEndedPromptTemplate\n",
    "from utility.constants import BigCodeBench, HumanEval, LexicalMutations, SyntacticMutations, LogicalMutations, PromptTypes, CodeGeneration, ReasoningModels, NonReasoningModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "## Declaring Benchmark Name Constants\n",
    "BIGCODEBENCH = BigCodeBench.NAME\n",
    "HUMANEVAL = HumanEval.NAME\n",
    "\n",
    "## Declaring Reasoning Model Name Constants\n",
    "GPT5 = ReasoningModels.GPT5['name']\n",
    "\n",
    "## Declaring Non-Reasoning Model Name Constants\n",
    "CODESTRAL = NonReasoningModels.CODESTRAL['name']\n",
    "GPT4O = NonReasoningModels.GPT4O['name']\n",
    "DEEPSEEK = NonReasoningModels.DEEPSEEK_CHAT['name']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_models = [getattr(ReasoningModels, model) for model in dir(ReasoningModels) if not model.startswith(\"_\")]\n",
    "non_reasoning_models = [getattr(NonReasoningModels, model) for model in dir(NonReasoningModels) if not model.startswith(\"_\")]\n",
    "print('Reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")\n",
    "print('=' * 50)\n",
    "print('Non-reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(non_reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = BIGCODEBENCH\n",
    "\n",
    "try:\n",
    "    llmtester = CodeGenerationTester(f\"{task_set}_Code_Generation\", n =5 )\n",
    "except Exception as e:\n",
    "    print(f'llmtester could not launch due to the following error: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = llmtester.question_database.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mutations = CodeGeneration.MUTATIONS\n",
    "print(\"These are the valid mutation names for code generation:\")\n",
    "for idx, mutation in enumerate(valid_mutations):\n",
    "    if mutation != LITERAL_FORMAT:\n",
    "        print(idx+1, mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Non-interactive backend (no GUI)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show = lambda *args, **kwargs: None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Run your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "`run_code_generation_test` method is used for running code generation tests on MuCoCo.\n",
    "\n",
    "| Parameter              | Type        | Description                                                                                                              |\n",
    "| ---------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `prompt_helper`        | `str`       | String template for the appropriate prompt. Simply rename the `prompt_type` variable to `ONE_SHOT` or `ZERO_SHOT` only.\n",
    "| `output_file_path`     | `str`       | Full path to the CSV where predictions and metrics are saved. Filename is built from model, task type, and mutation tag. |\n",
    "| `num_tests`            | `int`       | Number of test questions to evaluate. The number of questions available for evaluation ranges from 1 to 160.                                                                               |\n",
    "| `mutations`            | `List[str]` | Mutation operators to apply (e.g., `[\"FOR2WHILE\"]`, `[\"CONSTANT_UNFOLD\"]`). Empty list means **no_mutation**.            |\n",
    "| `model_name`           | `str`       | Identifier of the LLM under test (e.g., `GPT4O`). Used for routing and naming.                                           |\n",
    "| `task_set`      | `str`       | Either  `BIGCODEBENCH` or `HUMANEVAL` for code generation.                                                           |                          \n",
    "| `continue_from_task`   | `str`       | Optional parameter for starting evaluation from a specified task ID corresponding to the task in MongoDB (e.g., `\"BigCodeBencho15\"`).                                                |\n",
    "\n",
    "The following example runs a code generation test on the BigCodeBench benchmark for all tasks in BigCodeBench. To add mutations such as Random mutation, add the corresponding mutation string to the `mutations` list like so: `mutations = [RANDOM_MUTATION]`. The mutations available for code generation testing are declared as constants above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations = []\n",
    "prompt_type = ONE_SHOT\n",
    "model_name = GPT4O    # Change to your desired model.\n",
    "\n",
    "# Forming the results directory\n",
    "results_dir =os.path.join(proj_dir, f'results/code_generation/{model_name}')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "output_file_path=f\"{results_dir}/{task_set}_{prompt_type}_{mutation_str}.csv\"\n",
    "\n",
    "pass_count = llmtester.run_code_generation_test(\n",
    "    prompt_helper = OpenEndedPromptTemplate().return_appropriate_prompt(prompt_type),\n",
    "    num_tests=num_tests,\n",
    "    mutations = mutations,\n",
    "    prompt_type= prompt_type,\n",
    "    output_file_path=output_file_path,\n",
    "    task_set = task_set,\n",
    "    model_name= model_name,\n",
    ")\n",
    "\n",
    "print(fr\"Results saved in {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
