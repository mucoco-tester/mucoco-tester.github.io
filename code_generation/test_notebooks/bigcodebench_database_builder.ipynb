{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## MongoDB BigCodeBench Code Generation Database Builder\n",
    "\n",
    "This notebook is used to initialize a new code generation database onto MongoDB for BigCodeBench benchmark.\n",
    "\n",
    "It should be noted that there could be artifacts leftover from building the database. They will appear under the parent directory to this notebook. You may remove them once the database has finished generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Non-interactive backend (no GUI)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show = lambda *args, **kwargs: None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "bigcodebench = pd.read_csv(os.path.join(proj_dir, \"datasets/open_ended_format/bigcodebench_test.csv\"), header = 0, encoding='utf-8')\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_generation.utility.bigcodebench_helper import CodeGenerationBigCodeBenchHelper\n",
    "from database import MongoDBHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodbHelper = MongoDBHelper()\n",
    "mongodbHelper.check_database_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongodbHelper.client[os.getenv('MONGODB_BENCHMARK_DATABASE')]\n",
    "open_ended_db = db[os.getenv('MONGODB_BIGCODEBENCH_COLLECTION')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "to_check_example = []\n",
    "to_check_test = []\n",
    "\n",
    "try: \n",
    "    for idx in range(\n",
    "        bigcodebench.__len__(),\n",
    "        ):\n",
    "\n",
    "        qn_id = f\"BigCodeBencho{idx-len(to_check_example)-len(to_check_test)}\"\n",
    "\n",
    "        qn_details = open_ended_db.find_one({\"_id\" : qn_id})        # checking if this qn_id already exists in the db\n",
    "\n",
    "        qn = bigcodebench.iloc[idx]\n",
    "\n",
    "        original_prompt = qn['complete_prompt'].replace(r\"\\\\x\", r\"\\\\\\\\x\")\n",
    "        canonical_solution = qn['canonical_solution']\n",
    "        instruct_prompt = qn['instruct_prompt']\n",
    "\n",
    "        test = qn['test']\n",
    "        original_test_id = qn['task_id']\n",
    "\n",
    "        # storing all the file names in the current directory as a snapshot\n",
    "        # this is a necessary step to remove any new files created from running the tasks\n",
    "        snapshot_dir = os.listdir(curr_dir)\n",
    "\n",
    "        prompt, qn_desc = CodeGenerationBigCodeBenchHelper.seperate_original_desciptions(original_prompt)\n",
    "\n",
    "        task_doc_string, example = CodeGenerationBigCodeBenchHelper.extract_examples(qn_desc)\n",
    "\n",
    "        natural_language_instruct, code_instruct= CodeGenerationBigCodeBenchHelper.split_code_from_instruct_prompt(instruct_prompt=instruct_prompt)\n",
    "        try: \n",
    "            full_sol = CodeGenerationBigCodeBenchHelper.obtain_full_sol(\n",
    "                canonical_sol=canonical_solution, \n",
    "                code_instruct=code_instruct,\n",
    "                test = test)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(original_test_id)\n",
    "            to_check_test.append(original_test_id)\n",
    "            continue\n",
    "        \n",
    "        entry_dict = {\n",
    "            \"_id\" : qn_id,\n",
    "            \"qn\" : code_instruct,\n",
    "            \"qn_desc\": task_doc_string,\n",
    "            \"canon_solution\" : canonical_solution,\n",
    "            \"original_qn_desc\" : qn_desc,\n",
    "            \"examples\": example,\n",
    "            \"check\" : test,\n",
    "            \"original_id\": original_test_id\n",
    "        }\n",
    "\n",
    "        curr_dir_snapshot = os.listdir(curr_dir)\n",
    "        for file_name in curr_dir_snapshot:\n",
    "            if file_name not in snapshot_dir:\n",
    "                file_path = os.path.join(curr_dir, file_name)\n",
    "                if os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "                else:\n",
    "                    os.remove(file_path)\n",
    "\n",
    "\n",
    "        if qn_details is None:\n",
    "            open_ended_db.insert_one(entry_dict)\n",
    "            print('Added entry to database: {id}'.format(id = qn_id))\n",
    "        else:\n",
    "            open_ended_db.update_one({\"_id\" : qn_id}, update = {\"$set\": entry_dict})\n",
    "            print('Updated existing entry in database: {id}'.format(id = qn_id))\n",
    "except KeyboardInterrupt:\n",
    "    print(original_test_id)\n",
    "except Exception as e:\n",
    "    print(original_test_id)\n",
    "    print(e)\n",
    "    print(to_check_test)\n",
    "\n",
    "\n",
    "print(to_check_example if len(to_check_example) > 0 else \"All cases contains examples. Nothing to check!\")\n",
    "print(to_check_test if len(to_check_test) > 0 else \"All test cases passed. Nothing to check!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "During the time of preparing this notebook for submission, we found that the following BigCodeBench tasks could not be successfully processed:\n",
    "\n",
    "- `BigCodeBench/39`\n",
    "- `BigCodeBench/80`\n",
    "- `BigCodeBench/81`\n",
    "- `BigCodeBench/82`\n",
    "- `BigCodeBench/83`\n",
    "- `BigCodeBench/101`\n",
    "- `BigCodeBench/115`\n",
    "- `BigCodeBench/177`\n",
    "- `BigCodeBench/205`\n",
    "- `BigCodeBench/245`\n",
    "- `BigCodeBench/334`\n",
    "- `BigCodeBench/360`\n",
    "- `BigCodeBench/361`\n",
    "- `BigCodeBench/362`\n",
    "- `BigCodeBench/363`\n",
    "- `BigCodeBench/372`\n",
    "- `BigCodeBench/383`\n",
    "- `BigCodeBench/495`\n",
    "- `BigCodeBench/501`\n",
    "- `BigCodeBench/590`\n",
    "- `BigCodeBench/593`\n",
    "- `BigCodeBench/596`\n",
    "- `BigCodeBench/612`\n",
    "- `BigCodeBench/634`\n",
    "- `BigCodeBench/686`\n",
    "- `BigCodeBench/734`\n",
    "- `BigCodeBench/736`\n",
    "- `BigCodeBench/779`\n",
    "- `BigCodeBench/940`\n",
    "- `BigCodeBench/964`\n",
    "- `BigCodeBench/1005`\n",
    "- `BigCodeBench/1028`\n",
    "- `BigCodeBench/1084`\n",
    "- `BigCodeBench/1109`\n",
    "\n",
    "We suspect this is due to package conflicts and will require further investigation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
