{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Turbulence Benchmark Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The Turbulence benchmark is used to evaluate the robustness of LLMs in code generation tasks. This notebook uses the source code from the benchmark and adapts it for the use case of testing with MuCoCo. It is very computationally expensive to run this locally, so do consider to run it on a Virtual Machine to speed up the database build rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Iterable\n",
    "from tqdm import tqdm\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "par_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(par_dir)\n",
    "sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.helper_functions import TurbulenceBenchmarkHelper\n",
    "from database import MongoDBHelper\n",
    "from utility.constants import Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MongoDBHelper(max_retries= 5)\n",
    "if db.check_database_connectivity():\n",
    "    print(\"MongoDB connected\")\n",
    "\n",
    "base_qns_db = db.client[os.getenv('MONGODB_TURBULENCE_DATABASE')]\n",
    "baseline_db = base_qns_db[os.getenv('MONGODB_TURBULENCE_COLLECTION')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "source_code_dir = os.path.join(curr_dir, \"Source_Code (from Turbulence GitHub)\")\n",
    "qn_folders = [f for f in os.listdir(source_code_dir) if os.path.isdir(os.path.join(source_code_dir, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Processing Turbulence Dataset and storing into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%script false --no-raise-error\n",
    "\n",
    "seed = Seed.value\n",
    "random.seed(seed)\n",
    "\n",
    "failed_params = {}\n",
    "\n",
    "for qn_idx in tqdm(range(len(qn_folders))):\n",
    "    qn_folder_name = qn_folders[qn_idx]\n",
    "    # setting the correct qn_num\n",
    "    q_no = qn_folder_name.split(\"Q\")[-1]  \n",
    "\n",
    "    if q_no != \"\":\n",
    "\n",
    "        print(q_no)\n",
    "\n",
    "        func_name = None            # stores the task function name\n",
    "        \n",
    "        # obtaining the folder directory to the target qn. In this benchmark, each question is kept in an individual folder.\n",
    "        qn_folder_dir = os.path.join(source_code_dir, qn_folder_name)\n",
    "        \n",
    "        # initializing a TurbulenceBenchmarkHelper object with the question number and seed\n",
    "        helper = TurbulenceBenchmarkHelper(q_no= q_no, seed=seed)\n",
    "\n",
    "        # 1. Generating the params to substitute into the solutions etc. \n",
    "        gen_params_res = helper.run_gen_params(\n",
    "            qn_folder_dir = qn_folder_dir,\n",
    "        )\n",
    "\n",
    "        og_params_res = len(gen_params_res)\n",
    "\n",
    "        # Ensuring that each parameter generated is iterable, else it is converted to a Tuple.\n",
    "        # This step is necessary as some test functions require an iterable input\n",
    "        gen_params_res = [(param, ) if not isinstance(param, Iterable) else param for param in gen_params_res]\n",
    "\n",
    "        # 2. Using the generated params to generate the function inputs.\n",
    "        input_generator_res = helper.run_input_generator(\n",
    "            qn_folder_dir = qn_folder_dir,\n",
    "            gen_params = gen_params_res\n",
    "        )\n",
    "\n",
    "        og_gen_res = len(input_generator_res)\n",
    "\n",
    "        # 3. Obtaining the solution, natural language and test templates\n",
    "        sol_template = TurbulenceBenchmarkHelper.return_template_contents(\n",
    "            dir = os.path.join(qn_folder_dir, \"solution.py.template\")\n",
    "        )\n",
    "\n",
    "        prompt_template = TurbulenceBenchmarkHelper.return_template_contents(\n",
    "            dir = os.path.join(qn_folder_dir, \"question.txt.template\")\n",
    "        )\n",
    "\n",
    "        tests_template = TurbulenceBenchmarkHelper.return_template_contents(\n",
    "            dir = os.path.join(qn_folder_dir, \"tests.py.template\")\n",
    "        )\n",
    "\n",
    "        # Removing the unnecessary import statements in test template\n",
    "        tests_template = helper.process_test_cases(test_template=tests_template)\n",
    "\n",
    "        # iterating through each sample generated for each task\n",
    "        idx = 0\n",
    "        while idx < len(input_generator_res):\n",
    "            # If statement checking if the q_no is 29 and if it has exceeded index 9. Refer to the Turbulence failure documentation for the reason why an exception has to be made for q_no 29.\n",
    "            if q_no == \"29\" and idx >=9:\n",
    "                failed_params[q_no] = og_params_res - idx\n",
    "                gen_params_res = gen_params_res[:idx]\n",
    "                input_generator_res = input_generator_res[:idx]\n",
    "                break\n",
    "            \n",
    "            params = gen_params_res[idx]\n",
    "            func_input = input_generator_res[idx]\n",
    "\n",
    "            solution = sol_template\n",
    "            prompt = prompt_template\n",
    "            tests = tests_template\n",
    "            for param_idx, param in enumerate(params):\n",
    "                solution = solution.replace(f\"${param_idx}\", str(param))\n",
    "                tests = tests.replace(f\"${param_idx}\", str(param))\n",
    "            \n",
    "            if func_name is None:\n",
    "                try:\n",
    "                    func_name = helper.obtain_func_name(\n",
    "                        sol_template= solution,\n",
    "                        qn_txt_template= prompt_template\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    failed_params[q_no] = e\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "            tests = helper.replace_func_name(tests_template = tests, func_name = func_name)\n",
    "\n",
    "            try:\n",
    "                helper.run_test_suite(tests = tests, solution = solution)\n",
    "            except Exception as e:\n",
    "                gen_params_res.pop(idx)\n",
    "                input_generator_res.pop(idx)\n",
    "                failed_params[q_no] = failed_params.get(q_no, 0) + 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                ans = helper.obtain_canon_sol_output(\n",
    "                    solution = solution,\n",
    "                    test_input = func_input,\n",
    "                    func_name=func_name\n",
    "                )\n",
    "                \n",
    "                input_generator_res[idx] = (func_input, ans)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to obtain the canon solution output for {q_no}\")\n",
    "                pass\n",
    "\n",
    "            idx += 1\n",
    "        \n",
    "        # If statement checking if any params were failed\n",
    "        if failed_params.get(q_no, 0) != 0:\n",
    "            failed_params[q_no] = f\"{og_params_res - failed_params[q_no]}/{og_params_res}\"\n",
    "\n",
    "        # Ensuring that Q48, which has no parameters passing, does not get uploaded into the Turbulence database\n",
    "        if len(input_generator_res) == 0 or len(gen_params_res) == 0:\n",
    "            continue \n",
    "        \n",
    "        # Parameter and test inputs dictionary\n",
    "        param_dict = {}\n",
    "        for idx in range(len(gen_params_res)):\n",
    "            params_entry = gen_params_res[idx]\n",
    "            input_entry = input_generator_res[idx][0]\n",
    "            output_entry = input_generator_res[idx][1]\n",
    "\n",
    "\n",
    "            param_dict[str(idx)] = {\n",
    "                \"params\": helper.process_for_mongo_db_storage(params_entry),\n",
    "                \"func_input\" : helper.process_for_mongo_db_storage(input_entry),\n",
    "                \"func_output\": helper.process_for_mongo_db_storage(output_entry)\n",
    "            }\n",
    "        \n",
    "        task_id = f\"TurbulenceQ{qn_idx + 1}\"\n",
    "\n",
    "        # Creating the database entry\n",
    "        database_entry = {\n",
    "            \"_id\": task_id,\n",
    "            \"question_template\": tests_template,\n",
    "            \"prompt_template\": prompt_template,\n",
    "            \"solution_template\": sol_template,\n",
    "            \"func_name\": func_name,\n",
    "            \"params\": param_dict,\n",
    "            \"original_id\": f\"Q{q_no}\"\n",
    "        }\n",
    "        \n",
    "        # Storing entry into the baseline database\n",
    "        exisiting_entry = baseline_db.find_one({\"original_id\": f\"Q{q_no}\"})\n",
    "\n",
    "        try:\n",
    "            if exisiting_entry is not None:\n",
    "                baseline_db.find_one_and_replace({\"_id\": task_id}, database_entry)\n",
    "            else:\n",
    "                baseline_db.insert_one(database_entry)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Not all cases of Turbulence pass the check function. This is a list of questions from the original Turbulence database which did not fully pass the test suite. The reason for failure is included in a pdf located in ...\n",
    "\n",
    "Number of test cases passed for Q8: 93\n",
    "\n",
    "Number of test cases passed for Q13: 4\n",
    "\n",
    "Number of test cases passed for Q41: 99\n",
    "\n",
    "Number of test cases passed for Q48: 0\n",
    "\n",
    "Number of test cases passed for Q21: 97\n",
    "\n",
    "Number of test cases passed for Q29: 9\n",
    "\n",
    "Number of test cases passed for Q42: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Printing all tasks that failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "print(\"The following tasks failed the following parameters:\")\n",
    "for q_no, count in failed_params.items():\n",
    "    print(f\"    Q{q_no} - Canon solution pass count {count}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Retrieving documents stored in Turbulence datasets and checking test oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "This is a necessary step as it ensures that the experiment is still replicable and valid even after storing it into MongoDB. Any tests that fails this step should be removed from the MongoDB database as it is deemed an invalid test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = baseline_db.count_documents({})\n",
    "seed = Seed.value\n",
    "tot_tasks = 0\n",
    "\n",
    "for idx in range(60):\n",
    "\n",
    "    q_no = idx + 1\n",
    "    task_id = f\"TurbulenceQ{q_no}\"\n",
    "\n",
    "    doc = baseline_db.find_one({\"_id\": task_id})\n",
    "\n",
    "    if doc is None:\n",
    "        print(f\"Could not retrieve {task_id} from the dataset.\")\n",
    "        continue\n",
    "\n",
    "    qn_template = doc[\"question_template\"]\n",
    "    solution_template = doc[\"solution_template\"]\n",
    "    func_name = doc[\"func_name\"]\n",
    "    param_dict: dict = doc[\"params\"]\n",
    "    original_id: str = doc['original_id']\n",
    "\n",
    "    original_q_no = original_id.split(\"Q\")[-1]\n",
    "\n",
    "    for task in param_dict.values():\n",
    "        params = task['params']\n",
    "        func_input = task['func_input']\n",
    "        func_output = task['func_output']\n",
    "        solution = solution_template\n",
    "        tests = qn_template\n",
    "\n",
    "        helper = TurbulenceBenchmarkHelper(q_no= original_q_no, seed = seed)\n",
    "\n",
    "        processed_params = helper.convert_data_to_metadata(data = params[\"data\"], metadata = params['metadata'])\n",
    "\n",
    "        for param_idx, processed_param in enumerate(processed_params):\n",
    "            solution = solution.replace(f\"${param_idx}\", str(processed_param))\n",
    "            tests = tests.replace(f\"${param_idx}\", str(processed_param))\n",
    "\n",
    "        tests = helper.replace_func_name(tests_template = tests, func_name = func_name)\n",
    "\n",
    "        try:\n",
    "            helper.run_test_suite(tests = tests, solution = solution)\n",
    "        except Exception as e:\n",
    "            print(tests)\n",
    "            print(solution)\n",
    "            print(type(e), e)\n",
    "            print(f'{task_id} failed the tests when retrieved')\n",
    "            # baseline_db.find_one_and_delete({\"_id\": task_id})\n",
    "\n",
    "        try: \n",
    "            processed_func_input = helper.convert_data_to_metadata(func_input['data'], func_input['metadata'])\n",
    "            processed_func_output = helper.convert_data_to_metadata(func_output['data'], func_output['metadata'])\n",
    "            helper.verify_prog_answer(\n",
    "                canonical_sol=solution,\n",
    "                func_input=processed_func_input,\n",
    "                func_name = func_name,\n",
    "                func_output=processed_func_output,\n",
    "            )\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(solution)\n",
    "            print(type(e), e)\n",
    "            print(func_input, \"---\", func_output)\n",
    "            print(f'{task_id} canon solution returned a different output')\n",
    "    print(original_id, len(param_dict))\n",
    "    tot_tasks += len(param_dict)\n",
    "\n",
    "print(f\"A total of {tot_tasks} is available for testing.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
