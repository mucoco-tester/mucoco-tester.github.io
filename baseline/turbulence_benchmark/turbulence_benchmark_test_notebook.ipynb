{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Turbulence Benchmark Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(curr_dir)\n",
    "proj_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(proj_dir)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.constants import LexicalMutations, PromptTypes, CodeGeneration, ReasoningModels, NonReasoningModels, SamplingMethods, InputPrediction, OutputPrediction, SyntacticMutations, LogicalMutations, Turbulence\n",
    "from baseline.turbulence_benchmark.turbulence_tester import TurbulenceTester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring Prompt Type Constants\n",
    "ZERO_SHOT = PromptTypes.ZERO_SHOT\n",
    "ONE_SHOT = PromptTypes.ONE_SHOT\n",
    "FEW_SHOT = PromptTypes.FEW_SHOT\n",
    "\n",
    "## Declaring Mutation Constants\n",
    "FOR2WHILE = SyntacticMutations.FOR2WHILE\n",
    "FOR2ENUMERATE = SyntacticMutations.FOR2ENUMERATE\n",
    "\n",
    "RANDOM_MUTATION = LexicalMutations.RANDOM\n",
    "SEQUENTIAL_MUTATION = LexicalMutations.SEQUENTIAL\n",
    "LITERAL_FORMAT = LexicalMutations.LITERAL_FORMAT\n",
    "\n",
    "BOOLEAN_LITERAL = LogicalMutations.BOOLEAN_LITERAL\n",
    "DEMORGAN = LogicalMutations.DEMORGAN\n",
    "COMMUTATIVE_REORDER = LogicalMutations.COMMUTATIVE_REORDER\n",
    "CONSTANT_UNFOLD = LogicalMutations.CONSTANT_UNFOLD\n",
    "CONSTANT_UNFOLD_ADD = LogicalMutations.CONSTANT_UNFOLD_ADD\n",
    "CONSTANT_UNFOLD_MULT = LogicalMutations.CONSTANT_UNFOLD_MULT\n",
    "\n",
    "\n",
    "## Declaring Benchmark Name Constants\n",
    "TURBULENCE = Turbulence.NAME\n",
    "\n",
    "## Declaring Reasoning Model Name Constants\n",
    "GPT5 = ReasoningModels.GPT5['name']\n",
    "\n",
    "## Declaring Non-Reasoning Model Name Constants\n",
    "CODESTRAL = NonReasoningModels.CODESTRAL['name']\n",
    "GPT4O = NonReasoningModels.GPT4O['name']\n",
    "DEEPSEEK = NonReasoningModels.DEEPSEEK_CHAT['name']\n",
    "\n",
    "## Declaring Sampling Methods for Turbulence Dataset\n",
    "SYSTEMATIC = SamplingMethods.SYSTEMATIC\n",
    "RANDOM = SamplingMethods.RANDOM\n",
    "\n",
    "## Declaring Task Types\n",
    "INPUT_PREDICTION = InputPrediction.NAME\n",
    "OUTPUT_PREDICTION = OutputPrediction.NAME\n",
    "CODE_GENERATION = CodeGeneration.NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_models = [getattr(ReasoningModels, model) for model in dir(ReasoningModels) if not model.startswith(\"_\")]\n",
    "non_reasoning_models = [getattr(NonReasoningModels, model) for model in dir(NonReasoningModels) if not model.startswith(\"_\")]\n",
    "print('Reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")\n",
    "print('=' * 50)\n",
    "print('Non-reasoning models supported by this framework are:')\n",
    "for idx, model in enumerate(non_reasoning_models):\n",
    "    print(f\"{idx+1}: '{model['name']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = TURBULENCE\n",
    "\n",
    "try:\n",
    "    llmtester = TurbulenceTester(\n",
    "        qn_database= os.getenv('MONGODB_TURBULENCE_COLLECTION'),\n",
    "        base_db=os.getenv('MONGODB_TURBULENCE_DATABASE'),\n",
    "        n = 5\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f'llmtester could not launch due to the following error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = llmtester.question_database.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mutations = CodeGeneration.MUTATIONS\n",
    "print(\"These are the valid mutation names for code generation:\")\n",
    "for idx, mutation in enumerate(valid_mutations):\n",
    "    if mutation != LITERAL_FORMAT:\n",
    "        print(idx+1, mutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Run your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Two experiments on the Turbulence benchmark can be run from this notebook\n",
    "- Code Generation\n",
    "- Output/Input Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Turbulence Output and Input Prediction with MuCoCo\n",
    "\n",
    "Use ```run_prediction_inconsistency_test```. The following table outlines the valid parameters for this function.\n",
    "\n",
    "| Parameter              | Type        | Description                                                                                                              |\n",
    "| ---------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `output_file_path`     | `str`       | Full path to the CSV where predictions and metrics are saved. Filename is built from model, task type, and mutation tag. |\n",
    "| `num_tests`            | `int`       | Number of test turbulence questions to evaluate. Set to `num_tests` to run all Turbulence question templates.                                                   |\n",
    "| `mutations`            | `List[str]` | Mutation operators to apply (e.g., `[\"FOR2WHILE\"]`, `[\"CONSTANT_UNFOLD\"]`). Empty list means **no_mutation**.            |\n",
    "| `model_name`           | `str`       | Identifier of the LLM under test (e.g., `GPT4O`). Used for routing and naming.                                           |\n",
    "| `sampling_method`      | `str`       | Optional parameter method for `num_samples_per_task` test cases (e.g., `RANDOM` or `STRATIFIED`). Ensure that the `num_samples_per_task` is filled in to use with this parameter or remove it if running all templates.                                                                               |\n",
    "| `num_samples_per_task` | `int`       | Optional parameter of independent generations per task to assess consistency. Each Turbulence task have up to 100 tasks. Use this parameter to specify the sample of tasks to run. Use in conjunction with `sampling_method` variable or remove it if running all templates.                                                         |\n",
    "| `task_type`            | `str`       | Task to test on (e.g., `OUTPUT_PREDICTION` or `INPUT_PREDICTION`).                                |\n",
    "| `continue_from_task`   | `str`       | Optional parameter for starting evaluation from a specified task ID (e.g., `\"TurbulenceQ27\"`)                                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Output Prediction Example\n",
    "\n",
    "The following code sample runs a sample of Turbulence benchmark questions for all question templates (Q1 to Q60). The task type is output prediction and model used is GPT4O. A `random` sampling method is used to select `5` random samples to test on per question template.\n",
    "\n",
    "Do fill in the `.env` with the GPT4O API key as specified in `.env.example` and change the seed value in `.env` as desired. The default seed value is `1234`.\n",
    "\n",
    "To run the full Turbulence Output Prediction experiment, remove the `num_samples_per_task` and `sampling_method` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations = []\n",
    "prompt_type = ZERO_SHOT\n",
    "model_name = GPT4O\n",
    "\n",
    "# Forming the results directory\n",
    "results_dir =os.path.join(proj_dir, f'results/code_generation/{model_name}')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "output_file_path=f\"{results_dir}/{task_set}_{prompt_type}_{mutation_str}.csv\"  # modify the output file path here as desired.\n",
    "\n",
    "pass_count = llmtester.run_prediction_inconsistency_test(\n",
    "    output_file_path=output_file_path,\n",
    "    num_tests=num_tests,\n",
    "    mutations = mutations,\n",
    "    model_name= model_name,\n",
    "    sampling_method= RANDOM,\n",
    "    num_samples_per_task=5,\n",
    "    task_type = OUTPUT_PREDICTION,\n",
    ")\n",
    "\n",
    "print(fr\"Results saved in {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Input Prediction Example\n",
    "\n",
    "The following code sample runs a sample of Turbulence benchmark questions for all question templates (Q1 to Q60). The task type is input prediction and model used is GPT4O. A `random` sampling method is used to select `5` random samples to test on per question template.\n",
    "\n",
    "Do fill in the `.env` with the GPT4O API key as specified in `.env.example` and change the seed value in `.env` as desired. The default seed value is `1234`.\n",
    "\n",
    "To run the full Turbulence Input Prediction experiment, remove the `num_samples_per_task` and `sampling_method` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations = []\n",
    "prompt_type = ZERO_SHOT\n",
    "model_name = GPT4O\n",
    "\n",
    "# Forming the results directory\n",
    "results_dir =os.path.join(proj_dir, f'results/code_generation/{model_name}')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "output_file_path=f\"{results_dir}/{task_set}_{prompt_type}_{mutation_str}.csv\"  # modify the output file path here as desired.\n",
    "\n",
    "pass_count = llmtester.run_prediction_inconsistency_test(\n",
    "    output_file_path=output_file_path,\n",
    "    num_tests=3,\n",
    "    mutations = mutations,\n",
    "    model_name= model_name,\n",
    "    sampling_method= RANDOM,\n",
    "    num_samples_per_task=5,\n",
    "    task_type = INPUT_PREDICTION,\n",
    ")\n",
    "\n",
    "print(fr\"Results saved in {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Turbulence Code Generation with MuCoCo\n",
    "\n",
    "Use ```run_code_generation_test```. The following table outlines the valid parameters for this function.\n",
    "\n",
    "| Parameter              | Type        | Description                                                                                                              |\n",
    "| ---------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `output_file_path`     | `str`       | Full path to the CSV where predictions and metrics are saved. Filename is built from model, task type, and mutation tag. |\n",
    "| `num_tests`            | `int`       | Number of test turbulence questions to evaluate. Set to `num_tests` to run all question templates.                                                                               |\n",
    "| `mutations`            | `List[str]` | Mutation operators to apply (e.g., `[\"FOR2WHILE\"]`, `[\"CONSTANT_UNFOLD\"]`). Empty list means **no_mutation**.            |\n",
    "| `model_name`           | `str`       | Identifier of the LLM under test (e.g., `GPT4O`). Used for routing and naming.                                           |\n",
    "| `sampling_method`      | `str`       | How to sample test cases (e.g., `RANDOM` or `STRATIFIED`).                                                                               |\n",
    "| `num_samples_per_task` | `int`       | Number of independent generations per task to assess consistency. Each Turbulence task have up to 100 tasks. Use this parameter to specify the sample of tasks to run.                                                         |\n",
    "| `continue_from_task`   | `str`       | Optional parameter for starting evaluation from a specified task ID (e.g., `\"TurbulenceQ27\"`)                                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Code Generation Example\n",
    "\n",
    "The following code sample runs a sample of Turbulence benchmark questions for all question templates (Q1 to Q60). The task type is input prediction and model used is GPT4O. A `random` sampling method is used to select `5` random samples to test on per question template.\n",
    "\n",
    "Do fill in the `.env` with the GPT4O API key as specified in `.env.example` and change the seed value in `.env` as desired. The default seed value is `1234`.\n",
    "\n",
    "To run the full Turbulence Code Generation experiment, remove the `num_samples_per_task` and `sampling_method` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "mutations = []\n",
    "prompt_type = ZERO_SHOT\n",
    "model_name = GPT4O\n",
    "\n",
    "# Forming the results directory\n",
    "results_dir =os.path.join(proj_dir, f'results/code_generation/{model_name}')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "mutation_str = \"_\".join(mutations) if len(mutations) > 0 else \"no_mutation\"\n",
    "output_file_path=f\"{results_dir}/{task_set}_{prompt_type}_{mutation_str}.csv\"\n",
    "\n",
    "pass_count = llmtester.run_code_generation_test(\n",
    "    output_file_path=output_file_path,\n",
    "    num_tests=num_tests,\n",
    "    mutations = mutations,\n",
    "    model_name= model_name,\n",
    "    sampling_method= RANDOM,\n",
    "    num_samples_per_task=5,\n",
    ")\n",
    "\n",
    "print(fr\"Results saved in {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
